# Comparing `tmp/deepview_validator-3.0.3-py3-none-any.whl.zip` & `tmp/deepview_validator-3.0.4-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,46 +1,46 @@
-Zip file size: 97981 bytes, number of entries: 44
--rw-rw-r--  2.0 unx      760 b- defN 23-Apr-20 18:46 deepview/validator/__init__.py
--rw-rw-r--  2.0 unx    15914 b- defN 23-Apr-20 18:46 deepview/validator/__main__.py
--rw-rw-r--  2.0 unx    12847 b- defN 23-Apr-20 18:46 deepview/validator/exceptions.py
--rw-rw-r--  2.0 unx      497 b- defN 23-Apr-20 18:46 deepview/validator/datasets/__init__.py
--rw-rw-r--  2.0 unx    29989 b- defN 23-Apr-20 18:46 deepview/validator/datasets/core.py
--rw-rw-r--  2.0 unx    14505 b- defN 23-Apr-20 18:46 deepview/validator/datasets/darknet.py
--rw-rw-r--  2.0 unx     7029 b- defN 23-Apr-20 18:46 deepview/validator/datasets/tfrecord.py
--rw-rw-r--  2.0 unx     3786 b- defN 23-Apr-20 18:46 deepview/validator/datasets/utils.py
--rw-rw-r--  2.0 unx      530 b- defN 23-Apr-20 18:46 deepview/validator/evaluators/__init__.py
--rw-rw-r--  2.0 unx     5067 b- defN 23-Apr-20 18:46 deepview/validator/evaluators/core.py
--rw-rw-r--  2.0 unx    24515 b- defN 23-Apr-20 18:46 deepview/validator/evaluators/detectionevaluator.py
--rw-rw-r--  2.0 unx    18731 b- defN 23-Apr-20 18:46 deepview/validator/evaluators/segmentationevaluator.py
--rw-rw-r--  2.0 unx      830 b- defN 23-Apr-20 18:46 deepview/validator/metrics/__init__.py
--rw-rw-r--  2.0 unx    11297 b- defN 23-Apr-20 18:46 deepview/validator/metrics/core.py
--rw-rw-r--  2.0 unx    31668 b- defN 23-Apr-20 18:46 deepview/validator/metrics/detectiondata.py
--rw-rw-r--  2.0 unx    25023 b- defN 23-Apr-20 18:46 deepview/validator/metrics/detectionmetrics.py
--rw-rw-r--  2.0 unx     4548 b- defN 23-Apr-20 18:46 deepview/validator/metrics/detectionutils.py
--rw-rw-r--  2.0 unx    17211 b- defN 23-Apr-20 18:46 deepview/validator/metrics/segmentationdata.py
--rw-rw-r--  2.0 unx     6319 b- defN 23-Apr-20 18:46 deepview/validator/metrics/segmentationmetrics.py
--rw-rw-r--  2.0 unx    11704 b- defN 23-Apr-20 18:46 deepview/validator/metrics/segmentationutils.py
--rw-rw-r--  2.0 unx      674 b- defN 23-Apr-20 18:46 deepview/validator/runners/__init__.py
--rw-rw-r--  2.0 unx     5144 b- defN 23-Apr-20 18:46 deepview/validator/runners/core.py
--rw-rw-r--  2.0 unx    13596 b- defN 23-Apr-20 18:46 deepview/validator/runners/deepviewrt.py
--rw-rw-r--  2.0 unx    14335 b- defN 23-Apr-20 18:46 deepview/validator/runners/keras.py
--rw-rw-r--  2.0 unx     6147 b- defN 23-Apr-20 18:46 deepview/validator/runners/offline.py
--rw-rw-r--  2.0 unx    16285 b- defN 23-Apr-20 18:46 deepview/validator/runners/tensorrt.py
--rw-rw-r--  2.0 unx    15389 b- defN 23-Apr-20 18:46 deepview/validator/runners/tflite.py
--rw-rw-r--  2.0 unx      600 b- defN 23-Apr-20 18:46 deepview/validator/runners/modelclient/__init__.py
--rw-rw-r--  2.0 unx    29387 b- defN 23-Apr-20 18:46 deepview/validator/runners/modelclient/boxes.py
--rw-rw-r--  2.0 unx     3116 b- defN 23-Apr-20 18:46 deepview/validator/runners/modelclient/core.py
--rw-rw-r--  2.0 unx    11846 b- defN 23-Apr-20 18:46 deepview/validator/runners/modelclient/segmentation.py
--rw-rw-r--  2.0 unx      521 b- defN 23-Apr-20 18:46 deepview/validator/visualize/__init__.py
--rw-rw-r--  2.0 unx    11059 b- defN 23-Apr-20 18:46 deepview/validator/visualize/core.py
--rw-rw-r--  2.0 unx     7014 b- defN 23-Apr-20 18:46 deepview/validator/visualize/detectiondrawer.py
--rw-rw-r--  2.0 unx    14479 b- defN 23-Apr-20 18:46 deepview/validator/visualize/segmentationdrawer.py
--rw-rw-r--  2.0 unx      574 b- defN 23-Apr-20 18:46 deepview/validator/writers/__init__.py
--rw-rw-r--  2.0 unx     7611 b- defN 23-Apr-20 18:46 deepview/validator/writers/console.py
--rw-rw-r--  2.0 unx    19839 b- defN 23-Apr-20 18:46 deepview/validator/writers/core.py
--rw-rw-r--  2.0 unx     9065 b- defN 23-Apr-20 18:46 deepview/validator/writers/tensorboard.py
--rw-rw-r--  2.0 unx      433 b- defN 23-Apr-20 18:46 deepview_validator-3.0.3.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-Apr-20 18:46 deepview_validator-3.0.3.dist-info/WHEEL
--rw-rw-r--  2.0 unx       73 b- defN 23-Apr-20 18:46 deepview_validator-3.0.3.dist-info/entry_points.txt
--rw-rw-r--  2.0 unx        9 b- defN 23-Apr-20 18:46 deepview_validator-3.0.3.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     4268 b- defN 23-Apr-20 18:46 deepview_validator-3.0.3.dist-info/RECORD
-44 files, 434326 bytes uncompressed, 90995 bytes compressed:  79.0%
+Zip file size: 98203 bytes, number of entries: 44
+-rw-rw-r--  2.0 unx      760 b- defN 23-Apr-27 05:54 deepview/validator/__init__.py
+-rw-rw-r--  2.0 unx    16195 b- defN 23-Apr-27 05:54 deepview/validator/__main__.py
+-rw-rw-r--  2.0 unx    12847 b- defN 23-Apr-27 05:54 deepview/validator/exceptions.py
+-rw-rw-r--  2.0 unx      497 b- defN 23-Apr-27 05:54 deepview/validator/datasets/__init__.py
+-rw-rw-r--  2.0 unx    29989 b- defN 23-Apr-27 05:54 deepview/validator/datasets/core.py
+-rw-rw-r--  2.0 unx    14688 b- defN 23-Apr-27 05:54 deepview/validator/datasets/darknet.py
+-rw-rw-r--  2.0 unx     7029 b- defN 23-Apr-27 05:54 deepview/validator/datasets/tfrecord.py
+-rw-rw-r--  2.0 unx     3786 b- defN 23-Apr-27 05:54 deepview/validator/datasets/utils.py
+-rw-rw-r--  2.0 unx      530 b- defN 23-Apr-27 05:54 deepview/validator/evaluators/__init__.py
+-rw-rw-r--  2.0 unx     5067 b- defN 23-Apr-27 05:54 deepview/validator/evaluators/core.py
+-rw-rw-r--  2.0 unx    24515 b- defN 23-Apr-27 05:54 deepview/validator/evaluators/detectionevaluator.py
+-rw-rw-r--  2.0 unx    19208 b- defN 23-Apr-27 05:54 deepview/validator/evaluators/segmentationevaluator.py
+-rw-rw-r--  2.0 unx      830 b- defN 23-Apr-27 05:54 deepview/validator/metrics/__init__.py
+-rw-rw-r--  2.0 unx    11367 b- defN 23-Apr-27 05:54 deepview/validator/metrics/core.py
+-rw-rw-r--  2.0 unx    31668 b- defN 23-Apr-27 05:54 deepview/validator/metrics/detectiondata.py
+-rw-rw-r--  2.0 unx    25023 b- defN 23-Apr-27 05:54 deepview/validator/metrics/detectionmetrics.py
+-rw-rw-r--  2.0 unx     4548 b- defN 23-Apr-27 05:54 deepview/validator/metrics/detectionutils.py
+-rw-rw-r--  2.0 unx    17192 b- defN 23-Apr-27 05:54 deepview/validator/metrics/segmentationdata.py
+-rw-rw-r--  2.0 unx     6514 b- defN 23-Apr-27 05:54 deepview/validator/metrics/segmentationmetrics.py
+-rw-rw-r--  2.0 unx    11690 b- defN 23-Apr-27 05:54 deepview/validator/metrics/segmentationutils.py
+-rw-rw-r--  2.0 unx      735 b- defN 23-Apr-27 05:54 deepview/validator/runners/__init__.py
+-rw-rw-r--  2.0 unx     5144 b- defN 23-Apr-27 05:54 deepview/validator/runners/core.py
+-rw-rw-r--  2.0 unx    13520 b- defN 23-Apr-27 05:54 deepview/validator/runners/deepviewrt.py
+-rw-rw-r--  2.0 unx    17082 b- defN 23-Apr-27 05:54 deepview/validator/runners/keras.py
+-rw-rw-r--  2.0 unx     6147 b- defN 23-Apr-27 05:54 deepview/validator/runners/offline.py
+-rw-rw-r--  2.0 unx    16285 b- defN 23-Apr-27 05:54 deepview/validator/runners/tensorrt.py
+-rw-rw-r--  2.0 unx    14493 b- defN 23-Apr-27 05:54 deepview/validator/runners/tflite.py
+-rw-rw-r--  2.0 unx      612 b- defN 23-Apr-27 05:54 deepview/validator/runners/modelclient/__init__.py
+-rw-rw-r--  2.0 unx    27984 b- defN 23-Apr-27 05:54 deepview/validator/runners/modelclient/boxes.py
+-rw-rw-r--  2.0 unx     3116 b- defN 23-Apr-27 05:54 deepview/validator/runners/modelclient/core.py
+-rw-rw-r--  2.0 unx    11195 b- defN 23-Apr-27 05:54 deepview/validator/runners/modelclient/segmentation.py
+-rw-rw-r--  2.0 unx      521 b- defN 23-Apr-27 05:54 deepview/validator/visualize/__init__.py
+-rw-rw-r--  2.0 unx    11059 b- defN 23-Apr-27 05:54 deepview/validator/visualize/core.py
+-rw-rw-r--  2.0 unx     7014 b- defN 23-Apr-27 05:54 deepview/validator/visualize/detectiondrawer.py
+-rw-rw-r--  2.0 unx    14370 b- defN 23-Apr-27 05:54 deepview/validator/visualize/segmentationdrawer.py
+-rw-rw-r--  2.0 unx      574 b- defN 23-Apr-27 05:54 deepview/validator/writers/__init__.py
+-rw-rw-r--  2.0 unx     7611 b- defN 23-Apr-27 05:54 deepview/validator/writers/console.py
+-rw-rw-r--  2.0 unx    19839 b- defN 23-Apr-27 05:54 deepview/validator/writers/core.py
+-rw-rw-r--  2.0 unx     9065 b- defN 23-Apr-27 05:54 deepview/validator/writers/tensorboard.py
+-rw-rw-r--  2.0 unx      468 b- defN 23-Apr-27 05:54 deepview_validator-3.0.4.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-Apr-27 05:54 deepview_validator-3.0.4.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       73 b- defN 23-Apr-27 05:54 deepview_validator-3.0.4.dist-info/entry_points.txt
+-rw-rw-r--  2.0 unx        9 b- defN 23-Apr-27 05:54 deepview_validator-3.0.4.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     4268 b- defN 23-Apr-27 05:54 deepview_validator-3.0.4.dist-info/RECORD
+44 files, 435219 bytes uncompressed, 91217 bytes compressed:  79.0%
```

## zipnote {}

```diff
@@ -111,23 +111,23 @@
 
 Filename: deepview/validator/writers/core.py
 Comment: 
 
 Filename: deepview/validator/writers/tensorboard.py
 Comment: 
 
-Filename: deepview_validator-3.0.3.dist-info/METADATA
+Filename: deepview_validator-3.0.4.dist-info/METADATA
 Comment: 
 
-Filename: deepview_validator-3.0.3.dist-info/WHEEL
+Filename: deepview_validator-3.0.4.dist-info/WHEEL
 Comment: 
 
-Filename: deepview_validator-3.0.3.dist-info/entry_points.txt
+Filename: deepview_validator-3.0.4.dist-info/entry_points.txt
 Comment: 
 
-Filename: deepview_validator-3.0.3.dist-info/top_level.txt
+Filename: deepview_validator-3.0.4.dist-info/top_level.txt
 Comment: 
 
-Filename: deepview_validator-3.0.3.dist-info/RECORD
+Filename: deepview_validator-3.0.4.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## deepview/validator/__main__.py

```diff
@@ -2,16 +2,16 @@
 #
 # Unauthorized copying of this file, via any medium is strictly prohibited
 # Proprietary and confidential.
 #
 # This source code is provided solely for runtime interpretation by Python.
 # Modifying or copying any source code is explicitly forbidden.
 
-from deepview.validator.runners import DeepViewRTRunner, KerasRunner, \
-    OfflineRunner, TensorRTRunner
+from deepview.validator.runners import DeepViewRTRunner, DetectionKerasRunner,\
+    SegmentationKerasRunner, OfflineRunner, TensorRTRunner
 from deepview.validator.runners.modelclient import SegmentationDeepLab, \
     SegmentationModelPack
 from deepview.validator.runners.modelclient import BoxesModelPack, BoxesYolo
 from deepview.validator.exceptions import UnsupportedModelExtensionException
 from deepview.validator.exceptions import UnsupportedValidationTypeException
 from deepview.validator.exceptions import UnsupportedApplicationException
 from deepview.validator.evaluators import DetectionEval, SegmentationEval
@@ -319,22 +319,30 @@
 
         else:
             raise UnsupportedValidationTypeException(args.validate)
 
     # KERAS EVALUATION
     elif path.splitext(args.model)[1].lower() == ".h5":
 
-        runner = KerasRunner(
-            model_path=args.model,
-            labels=dataset.labels,
-            detection_iou_threshold=args.detection_iou,
-            detection_score_threshold=args.detection_threshold,
-            norm=args.norm,
-            label_offset=args.label_offset
-        )
+        if args.validate.lower() == "detection":
+
+            runner = DetectionKerasRunner(
+                model_path=args.model,
+                labels=dataset.labels,
+                detection_iou_threshold=args.detection_iou,
+                detection_score_threshold=args.detection_threshold,
+                norm=args.norm,
+                label_offset=args.label_offset
+            )
+        
+        elif args.validate.lower() == "segmentation":
+
+            runner = SegmentationKerasRunner(
+                model=args.model
+            )
 
     # TFLITE EVALUATION
     elif path.splitext(args.model)[1].lower() == ".tflite":
         raise NotImplementedError()
 
     # TensorRT Engine Evaluation
     elif path.splitext(args.model)[1].lower() == ".trt":
```

## deepview/validator/datasets/darknet.py

```diff
@@ -250,15 +250,16 @@
                 "Encountered a Gray image, skipping..{}".format(
                     basename(image_path)), code="WARNING")
             return None
 
         if self.validate_type == 'detection':
             load = self.txt_load_boxes(annotation_path)
         else:
-            load = self.json_load_polygon_instance(annotation_path)
+            load = self.json_load_polygon_instance(annotation_path, height, \
+                                                   width)
 
         if load is not None:
             boxes = load.get('boxes')
             labels = load.get('labels')
         else:
             return {
                 'image': image,
@@ -392,29 +393,34 @@
             return None
 
         return {
             'boxes': boxes,
             'labels': labels
         }
     # TODO: Make this method single with the method above.
-
-    def json_load_polygon_instance(self, annotation_path):
+    def json_load_polygon_instance(self, annotation_path, height, width):
         """
         This method loads a single instance from
         a JSON file in the image dataset to grab the segments.
         Unit-test for this method is defined under:
             file: test/deepview/validator/datasets/test_darknet.py
             function: test_json_load_boxes
 
         Parameters
         ----------
 
             annotation_path: str
                 This is the path to the JSON annotation
 
+            height: int
+                This is the image height.
+            
+            width: int
+                This is the image width.
+
         Returns
         -------
             annotation info: dict
                 This contains information such as:
 
                 .. code-block:: python
 
@@ -432,15 +438,14 @@
         """
 
         import json
 
         try:
             with open(annotation_path) as file:
                 data = json.load(file)
-                height, width = self.shape
                 try:
                     boxes = []
                     for polygon in data["segment"]:
                         # a list of vertices
                         x_y = []
                         for vertex in polygon:
                             vertex = self.denormalizer(
```

## deepview/validator/evaluators/segmentationevaluator.py

```diff
@@ -9,14 +9,15 @@
 from deepview.validator.metrics.segmentationutils import create_mask_image, \
     create_mask_class, create_mask_background, \
     classify_mask, create_mask_classes
 from deepview.validator.visualize import Drawer, SegmentationDrawer
 from deepview.validator.metrics import SegmentationDataCollection, \
     SegmentationMetrics
 from deepview.validator.evaluators.core import Evaluator
+from deepview.validator.datasets.core import Dataset
 from copy import deepcopy
 from os import path
 import numpy as np
 
 
 class SegmentationEval(Evaluator):
     """
@@ -184,20 +185,21 @@
                     }
 
         Raises
         ------
             None
         """
 
-        height, width = self.runner.get_input_shape()
         for gt_instance in self.dataset.read_all_samples():
 
             # This is for gray images that can't be processed.
             if gt_instance is None:
-                continue
+                continue    
+            height = gt_instance.get('height')
+            width = gt_instance.get('width')
 
             gt_mask = create_mask_image(height, width, gt_instance)
             gt_instance['gt_mask'] = gt_mask
             image = gt_instance.get("image")
             dt_mask = self.runner.run_single_instance(image)
             dt_labels = list(np.unique(dt_mask))
             dt_instance = {
@@ -206,30 +208,30 @@
             }
 
             yield {
                 'dt_instance': dt_instance,
                 'gt_instance': gt_instance
             }
 
-    def single_evaluation(self, instance, epoch=0, add_image=True):
+    def single_evaluation(self, instance, labels, epoch=0, add_image=True):
         """
         This method runs validation on a single instance.
         Unit-test for this method is defined under:
             file: test/deepview/validator/evaluators/test_segmentationevaluator.py
             function: test_single_evaluation
 
         Parameters
         ----------
 
             instance: dict
                 The ground truth and the predictions instances:
 
                 .. code-block:: python
 
-                    instances = {
+                    instance = {
                             'gt_instance': {
                                 'image': image numpy array,
                                 'height': height,
                                 'width': width,
                                 'gt_mask': ground truth mask of the image,
                                 'labels': list of labels,
                                 'image_path': image_path
@@ -255,24 +257,27 @@
                 total_tp, total_fn, total_class_fp, total_local_fp
 
         Raises
         ------
             None
         """
 
-        height, width = self.dataset.shape[0], self.dataset.shape[1]
+        height = instance.get('gt_instance').get('height')
+        width = instance.get('gt_instance').get('width')
         self.drawer = SegmentationDrawer()
         gt_labels = instance.get('gt_instance').get('labels')
         dt_labels = instance.get('dt_instance').get('labels')
         class_labels = np.unique(gt_labels + dt_labels)
 
-        self.datacollection.capture_class(class_labels, self.dataset.labels)
+        self.datacollection.capture_class(class_labels, labels)
 
         gt_mask = instance.get('gt_instance').get('gt_mask')
         dt_mask = instance.get('dt_instance').get('dt_mask')
+        dt_mask = Dataset.resize(dt_mask, (height, width))
+        instance['dt_instance']['dt_mask'] = dt_mask
         cu_tp_mask, cu_fp_mask, cu_fn_mask = None, None, None
 
         for cl in class_labels:
 
             gt_class_mask = create_mask_class(gt_mask, cl)
             dt_class_mask = create_mask_class(dt_mask, cl)
             # Evaluate background class
@@ -286,27 +291,26 @@
             cu_tp_mask = create_mask_classes(
                 tp_mask, cl, cu_tp_mask)
             cu_fp_mask = create_mask_classes(
                 fp_mask, cl, cu_fp_mask)
             cu_fn_mask = create_mask_classes(
                 fn_mask, cl, cu_fn_mask)
 
-            datalabel = self.datacollection.get_label_data(self.dataset.labels[cl])
+            datalabel = self.datacollection.get_label_data(labels[cl])
             datalabel.add_tp(tp)
             datalabel.add_fp(fp)
             datalabel.add_fn(fn)
             datalabel.add_gt(tp + fn)
 
-        # Execute this code for two pane image results
-        # dst = mask2maskimage(height, width, instances)
-
         # Execute this code for four pane image results
         if add_image:
-            dst = self.drawer.mask2mask4panes(
-                height, width, instance, cu_tp_mask, cu_fp_mask, cu_fn_mask)
+            # Execute this code for two pane image results
+            dst = self.drawer.mask2maskimage(instance)
+            # dst = self.drawer.mask2mask4panes(
+            #     instance, cu_tp_mask, cu_fp_mask, cu_fn_mask)
 
             if self.visualize:
                 n_path = path.join(self.save_path, path.basename(
                     instance.get('gt_instance').get('image_path')))
                 dst.save(n_path)
             elif self.tensorboardwriter:
                 self.tensorboardwriter(dst, instance.get(
@@ -333,28 +337,32 @@
 
         Raises
         ------
             None
         """
 
         self.counter = 0
-        height, width = self.dataset.shape[0], self.dataset.shape[1]
+        #height, width = self.dataset.shape[0], self.dataset.shape[1]
         self.drawer = SegmentationDrawer()
 
-        for instances in self.instance_collector():
+        for instance in self.instance_collector():
 
-            gt_labels = instances.get('gt_instance').get('labels')
-            dt_labels = instances.get('dt_instance').get('labels')
+            gt_labels = instance.get('gt_instance').get('labels')
+            dt_labels = instance.get('dt_instance').get('labels')
             class_labels = np.unique(gt_labels + dt_labels)
 
             self.datacollection.capture_class(
                 class_labels, self.dataset.labels)
 
-            gt_mask = instances.get('gt_instance').get('gt_mask')
-            dt_mask = instances.get('dt_instance').get('dt_mask')
+            height = instance.get('gt_instance').get('height')
+            width = instance.get('gt_instance').get('width')
+            gt_mask = instance.get('gt_instance').get('gt_mask')
+            dt_mask = instance.get('dt_instance').get('dt_mask')
+            dt_mask = Dataset.resize(dt_mask, (height, width))
+            instance['dt_instance']['dt_mask'] = dt_mask
             cu_tp_mask, cu_fp_mask, cu_fn_mask = None, None, None
 
             for cl in class_labels:
 
                 gt_class_mask = create_mask_class(gt_mask, cl)
                 dt_class_mask = create_mask_class(dt_mask, cl)
                 # Evaluate background class
@@ -376,32 +384,32 @@
                     self.dataset.labels[cl])
                 datalabel.add_tp(tp)
                 datalabel.add_fp(fp)
                 datalabel.add_fn(fn)
                 datalabel.add_gt(tp + fn)
 
             # Execute this code for two pane image results
-            #dst = self.drawer.mask2maskimage(height, width, instances)
+            dst = self.drawer.mask2maskimage(instance)
 
             # Execute this code for four pane image results
-            dst = self.drawer.mask2mask4panes(
-                height, width, instances, cu_tp_mask, cu_fp_mask, cu_fn_mask)
+            # dst = self.drawer.mask2mask4panes(
+            #    instance, cu_tp_mask, cu_fp_mask, cu_fn_mask)
 
             if self.display >= 0:
                 if self.counter < self.display:
                     self.counter += 1
                 else:
                     continue
 
             if self.visualize:
                 n_path = path.join(self.save_path, path.basename(
-                    instances.get('gt_instance').get('image_path')))
+                    instance.get('gt_instance').get('image_path')))
                 dst.save(n_path)
             elif self.tensorboardwriter:
-                self.tensorboardwriter(dst, instances.get(
+                self.tensorboardwriter(dst, instance.get(
                     'gt_instance').get('image_path'))
             else:
                 continue
 
         return self.datacollection.sum_outcomes()
 
     def conclude(self, truth_values, epoch=0):
@@ -451,15 +459,16 @@
         ------
             None
         """
 
         metrics = SegmentationMetrics(
             segmentationdatacollection=self.datacollection)
         timings = self.runner.summarize()
-        self.parameters["engine"] = self.runner.device
+        if self.parameters is not None:
+            self.parameters["engine"] = self.runner.device
 
         try:
             model_name = path.basename(self.runner.source)
         except AttributeError:
             model_name = "Training Model"
 
         try:
```

## deepview/validator/metrics/core.py

```diff
@@ -179,21 +179,21 @@
             ValueError
                 This method will raise the exception if the
                 provided parameters for tp and fp
                 are not integers or the values for tp and fp
                 are negative integers.
         """
 
-        if not isinstance(tp, (int, np.int32)):
+        if not isinstance(tp, (int, np.int32, np.int64)):
             raise ValueError(
                 "Unexpected input type of tp is provided. " +
                 "It should be type int. " +
                 "Provided with tp: {} ".format(type(tp))
             )
-        elif not isinstance(fp, (int, np.int32)):
+        elif not isinstance(fp, (int, np.int32, np.int64)):
             raise ValueError(
                 "Unexpected input type of fp is provided. " +
                 "It should be type int. " +
                 "Provided with fp: {} ".format(type(fp))
             )
         else:
             if (tp < 0 or fp < 0):
@@ -241,21 +241,21 @@
             ValueError
                 This method will raise the exception
                 if the provided parameters for tp and fn
                 are not integers or the values for tp
                 and fn are negative integers.
         """
 
-        if not isinstance(tp, (int, np.int32)):
+        if not isinstance(tp, (int, np.int32, np.int64)):
             raise ValueError(
                 "Unexpected input type of tp is provided. " +
                 "It should be type int. " +
                 "Provided with tp: {} ".format(type(tp))
             )
-        elif not isinstance(fn, (int, np.int32)):
+        elif not isinstance(fn, (int, np.int32, np.int64)):
             raise ValueError(
                 "Unexpected input type of fn is provided. " +
                 "It should be type int. " +
                 "Provided with fn: {} ".format(type(fn))
             )
         else:
             if (tp < 0 or fn < 0):
@@ -306,27 +306,27 @@
             ValueError
                 This method will raise the exception
                 if the provided parameters for tp, fp, and fn
                 are not integers or the values for tp, fp,
                 or fn are negative integers.
         """
 
-        if not isinstance(tp, (int, np.int32)):
+        if not isinstance(tp, (int, np.int32, np.int64)):
             raise ValueError(
                 "Unexpected input type of tp is provided. " +
                 "It should be type int. " +
                 "Provided with tp: {}".format(type(tp))
             )
-        elif not isinstance(fp, (int, np.int32)):
+        elif not isinstance(fp, (int, np.int32, np.int64)):
             raise ValueError(
                 "Unexpected input type of fp is provided. " +
                 "It should be type int. " +
                 "Provided with fp: {}".format(type(fp))
             )
-        elif not isinstance(fn, (int, np.int32)):
+        elif not isinstance(fn, (int, np.int32, np.int64)):
             raise ValueError(
                 "Unexpected input type of fn is provided. " +
                 "It should be type int. " +
                 "Provided with fn: {}".format(type(fn))
             )
         else:
             if (tp < 0 or fp < 0 or fn < 0):
```

## deepview/validator/metrics/segmentationdata.py

```diff
@@ -63,15 +63,15 @@
         Raises
         ------
             ValueError
                 This method will raise an exception if the input
                 parameter is not of type integer.
         """
 
-        if not (isinstance(num, int) or isinstance(num, np.int32)):
+        if not (isinstance(num, (int, np.int32, np.int64))):
             raise ValueError(
                 "The provided num has an incorrect type: {}. ".format(
                     type(num)) + "Can only accept integer type.")
         else:
             return num
 
     def add_gt(self, num=1):
@@ -261,15 +261,15 @@
             ValueError
                 This method will raise an exception
                 if the parameter for class_labels
                 does not contain integers.
         """
 
         for label_id in class_labels:
-            if (isinstance(label_id, int) or isinstance(label_id, np.int32)):
+            if (isinstance(label_id, (int, np.int32, np.int64))):
                 label = labels[label_id]
                 if label == " ":
                     continue
                 else:
                     if label not in self.labels:
                         self.add_label_data(label)
                         self.labels.append(label)
```

## deepview/validator/metrics/segmentationmetrics.py

```diff
@@ -150,27 +150,32 @@
         if num_class == 0:
             raise ZeroUniqueLabelsException()
         else:
             class_histogram_data = dict()
 
         for label_data in self.segmentationdatacollection.label_data_list:
 
-            precision = self.compute_precision(
-                label_data.tps,
-                label_data.fps
-            )
-            recall = self.compute_recall(
-                label_data.tps,
-                label_data.fns
-            )
-            accuracy = self.compute_accuracy(
-                label_data.tps,
-                label_data.fps,
-                label_data.fns
-            )
+            if label_data.tps == 0:
+                precision = 0.
+                recall = 0.
+                accuracy = 0.
+            else:
+                precision = self.compute_precision(
+                    label_data.tps,
+                    label_data.fps
+                )
+                recall = self.compute_recall(
+                    label_data.tps,
+                    label_data.fns
+                )
+                accuracy = self.compute_accuracy(
+                    label_data.tps,
+                    label_data.fps,
+                    label_data.fns
+                )
 
             mmap += precision
             mar += recall
             macc += accuracy
 
             class_histogram_data[label_data.get_label()] = {
                 'precision': precision,
```

## deepview/validator/metrics/segmentationutils.py

```diff
@@ -175,15 +175,15 @@
         ValueError
             This function will raise an exception if the input
             mask is not of type np.ndarray or if the input
             cls is not of type Integer.
     """
 
     mask = validate_input_mask(mask)
-    if not (isinstance(cls, int) or isinstance(cls, np.int32)):
+    if not (isinstance(cls, (int, np.int32, np.int64))):
         raise ValueError("The provided input parameter cls is not an integer. "
                          "Recieved type: {}".format(type(cls)))
     else:
         temp_mask = np.where(mask != cls, 0, mask)
         temp_mask[temp_mask == cls] = 1
         return temp_mask
 
@@ -224,15 +224,15 @@
             cls is not of type Integer.
     """
 
     new_mask = validate_input_mask(new_mask)
     if current_mask is not None:
         current_mask = validate_input_mask(current_mask)
 
-    if not (isinstance(cls, int) or isinstance(cls, np.int32)):
+    if not (isinstance(cls, (int, np.int32, np.int64))):
         raise ValueError("The provided input parameter cls is not an integer. "
                          "Recieved type: {}".format(type(cls)))
     else:
         new_mask = np.where(new_mask == 1, cls, new_mask)
         if current_mask is not None:
             return np.add(current_mask, new_mask)
         else:
```

## deepview/validator/runners/__init__.py

```diff
@@ -3,12 +3,13 @@
 # Unauthorized copying of this file, via any medium is strictly prohibited
 # Proprietary and confidential.
 #
 # This source code is provided solely for runtime interpretation by Python.
 # Modifying or copying any source code is explicitly forbidden.
 
 from deepview.validator.runners.deepviewrt import DeepViewRTRunner
+from deepview.validator.runners.keras import DetectionKerasRunner, \
+    SegmentationKerasRunner, InferenceKerasModel
 from deepview.validator.runners.tensorrt import TensorRTRunner
 from deepview.validator.runners.offline import OfflineRunner
 from deepview.validator.runners.tflite import TFliteRunner
-from deepview.validator.runners.keras import KerasRunner
 from deepview.validator.runners.core import Runner
```

## deepview/validator/runners/deepviewrt.py

```diff
@@ -224,15 +224,14 @@
                 (image, np.zeros((image.shape[0], image.shape[1], 1))), axis=2)
             self.ctx.load_image(rgba_image.astype(np.uint8))
             load_ns = clock_now() - start
         else:
             raise ValueError(
                 "The provided image is neither a path nor a np.ndarray. " +
                 "Provided with type: {}".format(type(image)))
-
         self.loading_input_timings.append(load_ns * 1e-6)
 
         start = clock_now()
         self.ctx.run_model()
         infer_ns = clock_now() - start
         self.inference_timings.append(infer_ns * 1e-6)
 
@@ -292,15 +291,14 @@
                     try:
                         dt_class = self.ctx.labels[label].lower().rstrip(
                             '\"').lstrip('\"')
                     except IndexError:
                         raise NonMatchingIndexException(label)
                     classes.append(dt_class)
                 else:
-                    # raise ModelUnknownLabelsException(model=self.source)
                     try:
                         dt_class = self.labels[label].lower().rstrip(
                             '\"').lstrip('\"')
                     except IndexError:
                         raise NonMatchingIndexException(label)
                     classes.append(dt_class)
             else:
```

## deepview/validator/runners/keras.py

```diff
@@ -8,19 +8,18 @@
 
 from deepview.validator.exceptions import UnsupportedNormalizationException
 from deepview.validator.exceptions import NonMatchingIndexException
 from deepview.validator.exceptions import MissingLibraryException
 from deepview.validator.datasets.core import Dataset
 from deepview.validator.runners.core import Runner
 from time import monotonic_ns as clock_now
-from os.path import exists
 import numpy as np
 
 
-class KerasRunner(Runner):
+class DetectionKerasRunner(Runner):
     """
     This class runs the Keras (h5) models using the tensorflow library.
     Unit-test for this class is defined under:
         file: test/deepview/validator/runners/test_keras.py
 
      Parameters
     ----------
@@ -73,15 +72,15 @@
             labels,
             detection_iou_threshold=0.5,
             detection_score_threshold=0.5,
             norm='unsigned',
             label_offset=0,
             max_detections=100
     ):
-        super(KerasRunner, self).__init__(model_path)
+        super(DetectionKerasRunner, self).__init__(model_path)
 
         self.iou_threshold = detection_iou_threshold
         self.score_threshold = detection_score_threshold
         self.norm = norm.lower()
         self.labels = labels
         self.device = "cpu"
         self.label_offset = label_offset
@@ -155,33 +154,18 @@
             ValueError
                 This method will raise an exception if the provided image is
                 neither a string path that points to the image nor is it a
                 numpy.ndarray. Furthermore it will raise an exception if the
                 provided image path does not exist.
         """
 
-        if isinstance(image, str):
-            start = clock_now()
-            if exists(image):
-                image = Dataset.resize(image, self.get_input_shape())
-                input_tensor = self.preprocess_input(image)
-            else:
-                raise ValueError(
-                    "The provided image path does not exist at: {}".format(
-                        image))
-            load_ns = clock_now() - start
-        elif isinstance(image, np.ndarray):
-            start = clock_now()
-            image = Dataset.resize(image, self.get_input_shape())
-            input_tensor = self.preprocess_input(image)
-            load_ns = clock_now() - start
-        else:
-            raise ValueError(
-                "The provided image is neither a path nor a np.ndarray. " +
-                "Provided with type: {}".format(type(image)))
+        start = clock_now()
+        image = Dataset.resize(image, self.get_input_shape())
+        input_tensor = self.preprocess_input(image)
+        load_ns = clock_now() - start
         self.loading_input_timings.append(load_ns * 1e-6)
 
         start = clock_now()
         outputs = self.model.predict(input_tensor, verbose=0)
         infer_ns = clock_now() - start
         self.inference_timings.append(infer_ns * 1e-6)
 
@@ -367,17 +351,136 @@
                 The model input shape.
 
         Raises
         ------
             None
         """
         return self.model.input.shape[1:]
+    
+
+class SegmentationKerasRunner(Runner):
+    """
+    This class runs Keras models to produce 
+    segmentation masks.
+    Unit-test for this class is defined under:
+        file: test/test_runners.py
+    
+    Parameters
+    -----------
+
+        model_path: str
+            The path to the Keras model.
+
+    
+    Raises
+    ------
+        MissingLibraryException:
+            This exception will be raised if the the tensorflow library
+            which is used to load and run a keras model is not installed.
+    """
+
+    def __init__(
+            self, 
+            model
+        ):
+        super(SegmentationKerasRunner, self).__init__(source=model)
+
+        try:
+            import tensorflow as tf
+        except ImportError:
+            raise MissingLibraryException(
+                "Tensorflow i needed to load Keras models."
+            )
+        
+        if isinstance(model, str):
+            self.model = tf.keras.models.load_model(model)
+        else:
+            self.model = model
+
+        self.device = "cpu"
+
+    def run_single_instance(self, image):
+        """
+        This method runs the loaded Keras model on a single
+        image to produce a mask for the image.
+        Unit-test for this method is defined under:
+            file: test/test_runners.py
+            function: test_run_single_instance
+
+        Parameters
+        ----------
+
+            image: str or np.ndarray
+                If the dataset is Darknet, then the image path is used which
+                is a string.
+                If the dataset is TFRecords, then the image is a
+                np.ndarray.
+
+        Returns
+        -------
+            mask: np.ndarray
+                This is the segmentation mask of the image 
+                where each pixel is represented by a class in
+                the image.
+
+        Raises
+        ------
+            MissingLibraryException
+                This method will raise an exception if the 
+                tensorflow library is not installed which is needed
+                to run a Keras model.
+        """
+
+        try:
+            import tensorflow as tf
+        except ImportError:
+            raise MissingLibraryException(
+                "Tensorflow i needed to load Keras models."
+            )
+
+        start = clock_now()
+        img = Dataset.resize(image, self.get_input_shape())
+        tensor = np.expand_dims(img, 0).astype(np.float32) / 255.
+        load_ns = clock_now() - start
+        self.loading_input_timings.append(load_ns * 1e-6)
+    
+        start = clock_now()
+        output = self.model.predict(tensor)
+        infer_ns = clock_now() - start
+        self.inference_timings.append(infer_ns * 1e-6)
+
+        start = clock_now()
+        mask = tf.argmax(output, axis=-1)[0].numpy().astype(np.uint8)
+        boxes_ns = clock_now() - start
+        self.box_timings.append(boxes_ns * 1e-6)
+
+        return mask
+
+    def get_input_shape(self):
+        """
+        This method returns the input shape of the 
+        Keras model.
+        Unit-test for this method is defined under:
+            file: test/test_runners.py
+            function: test_get_input_shape
+
+        Parameters
+        ----------
+            None
+        
+        Returns
+        -------
+            input shape: tuple
+                This is the model input shape (height, width)
+        """
+        _, mH, mW, num_classes = self.model.input.shape
+        return (mH, mW)
 
 
-class InferenceKerasModel(KerasRunner):
+class InferenceKerasModel(DetectionKerasRunner):
     """
     This class provides inference to the Keras Runner during model training.
     Unit-test for this class is defined under:
         file: test/deepview/validator/runners/test_keras.py
 
      Parameters
     ----------
```

## deepview/validator/runners/tflite.py

```diff
@@ -195,41 +195,23 @@
             ValueError
                 This method will raise an exception if the provided image is
                 neither a string path that points to the image nor is it a
                 numpy.ndarray. Furthermore it will raise an exception if the
                 provided image path does not exist.
         """
 
-        if isinstance(image, str):
-            start = clock_now()
-            if exists(image):
-                image = Dataset.resize(image, self.get_input_shape())
-                img = self.preprocess_input(image)
-                input_data = np.expand_dims(img, axis=0).astype(np.uint8)
-                self.interpreter.set_tensor(
-                    self.input_details[0]['index'],
-                    input_data
-                )
-            else:
-                raise ValueError(
-                    "The provided image path does not exist at: {}".format(
-                        image))
-            load_ns = clock_now() - start
-        elif isinstance(image, np.ndarray):
-            start = clock_now()
-            image = Dataset.resize(image, self.get_input_shape())
-            img = self.preprocess_input(image)
-            input_data = np.expand_dims(img, axis=0).astype(np.uint8)
-            self.interpreter.set_tensor(self.input_details[0]['index'],
-                                        input_data)
-            load_ns = clock_now() - start
-        else:
-            raise ValueError(
-                "The provided image is neither a path nor a np.ndarray. " +
-                "Provided with type: {}".format(type(image)))
+        start = clock_now()
+        image = Dataset.resize(image, self.get_input_shape())
+        img = self.preprocess_input(image)
+        input_data = np.expand_dims(img, axis=0).astype(np.uint8)
+        self.interpreter.set_tensor(
+            self.input_details[0]['index'],
+            input_data
+        )
+        load_ns = clock_now() - start
         self.loading_input_timings.append(load_ns)
 
         start = clock_now()
         self.interpreter.invoke()
         infer_ns = clock_now() - start
         self.inference_timings.append(infer_ns)
```

## deepview/validator/runners/modelclient/__init__.py

```diff
@@ -2,10 +2,12 @@
 #
 # Unauthorized copying of this file, via any medium is strictly prohibited
 # Proprietary and confidential.
 #
 # This source code is provided solely for runtime interpretation by Python.
 # Modifying or copying any source code is explicitly forbidden.
 
-from deepview.validator.runners.modelclient.segmentation import SegmentationRunner, SegmentationModelPack, SegmentationDeepLab
-from deepview.validator.runners.modelclient.boxes import BoxesModelPack, BoxesYolo
+from deepview.validator.runners.modelclient.segmentation import \
+    SegmentationRunner, SegmentationModelPack, SegmentationDeepLab
+from deepview.validator.runners.modelclient.boxes import BoxesModelPack, \
+    BoxesYolo
 from deepview.validator.runners.modelclient.core import ModelClientRunner
```

## deepview/validator/runners/modelclient/boxes.py

```diff
@@ -131,33 +131,18 @@
             ValueError
                 This method will raise an exception if the provided image is
                 neither a string path that points to the image nor is it a
                 numpy.ndarray. Furthermore it will raise an exception if the
                 provided image path does not exist.
         """
 
-        if isinstance(image, str):
-            start = clock_now()
-            if exists(image):
-                image = Dataset.resize(image, (self.shape[0], self.shape[1]))
-                data = np.expand_dims(image, 0).astype(np.uint8)
-            else:
-                raise ValueError(
-                    "The provided image path does not exist at: {}".format(
-                        image))
-            load_ns = clock_now() - start
-        elif isinstance(image, np.ndarray):
-            start = clock_now()
-            image = Dataset.resize(image, (self.shape[0], self.shape[1]))
-            data = np.expand_dims(image, 0).astype(np.uint8)
-            load_ns = clock_now() - start
-        else:
-            raise ValueError(
-                "The provided image is neither a path nor a np.ndarray. " +
-                "Provided with type: {}".format(type(image)))
+        start = clock_now()
+        image = Dataset.resize(image, (self.shape[0], self.shape[1]))
+        data = np.expand_dims(image, 0).astype(np.uint8)
+        load_ns = clock_now() - start
         self.loading_input_timings.append(load_ns * 1e-6)
 
         start = clock_now()
         response = self.client.run(
             {
                 'serving_default_input_1:0': data
             },
@@ -524,35 +509,19 @@
             ValueError
                 This method will raise an exception if the provided image is
                 neither a string path that points to the image nor is it a
                 numpy.ndarray. Furthermore it will raise an exception if the
                 provided image path does not exist.
         """
 
-        if isinstance(image, str):
-            start = clock_now()
-            if exists(image):
-                shape = (self.shape[:2])
-                image = Dataset.resize(image, (shape[1], shape[0]))
-                data = np.expand_dims(image, 0).astype(np.uint8)
-            else:
-                raise ValueError(
-                    "The provided image path does not exist at: {}".format(
-                        image))
-            load_ns = clock_now() - start
-        elif isinstance(image, np.ndarray):
-            start = clock_now()
-            shape = (self.shape[:2])
-            image = Dataset.resize(image, (shape[1], shape[0]))
-            data = np.expand_dims(image, 0).astype(np.uint8)
-            load_ns = clock_now() - start
-        else:
-            raise ValueError(
-                "The provided image is neither a path nor a np.ndarray. " +
-                "Provided with type: {}".format(type(image)))
+        start = clock_now()
+        shape = (self.shape[:2])
+        image = Dataset.resize(image, (shape[1], shape[0]))
+        data = np.expand_dims(image, 0).astype(np.uint8)
+        load_ns = clock_now() - start
         self.loading_input_timings.append(load_ns * 1e-6)
 
         start = clock_now()
         self.parameters = self.read_quant_parameters(self.target)
 
         response = self.client.run(
             {self.parameters["input_name"]: data},
```

## deepview/validator/runners/modelclient/segmentation.py

```diff
@@ -117,34 +117,19 @@
         """
 
         try:
             import cv2
         except ImportError:
             raise MissingLibraryException(
                 "opencv is needed to perform mask operations.")
-
-        if isinstance(image, str):
-            start = clock_now()
-            if exists(image):
-                img = Dataset.resize(image, self.get_input_shape())
-                inp = self.preprocess_input(img)
-            else:
-                raise ValueError(
-                    "The provided image path does not exist at: {}".format(
-                        image))
-            load_ns = clock_now() - start
-        elif isinstance(image, np.ndarray):
-            start = clock_now()
-            img = Dataset.resize(image, self.get_input_shape())
-            inp = self.preprocess_input(img)
-            load_ns = clock_now() - start
-        else:
-            raise ValueError(
-                "The provided image is neither a path nor a np.ndarray. " +
-                "Provided with type: {}".format(type(image)))
+        
+        start = clock_now()
+        img = Dataset.resize(image, self.get_input_shape())
+        inp = self.preprocess_input(img)
+        load_ns = clock_now() - start
         self.loading_input_timings.append(load_ns * 1e-6)
 
         start = clock_now()
         outputs = self.client.run(
             {
                 self.input_name: inp
             },
@@ -163,15 +148,14 @@
             interpolation=cv2.INTER_NEAREST)
         if self.seg_type.lower() == "modelpack":
             dt_mask = cv2.warpAffine(dt_mask.astype(np.uint8), np.float32(
                 [[1, 0, 15], [0, 1, 15]]), self.get_input_shape())
 
         boxes_ns = clock_now() - start
         self.box_timings.append(boxes_ns * 1e-6)
-
         return dt_mask
 
 
 class SegmentationModelPack(SegmentationRunner):
     """
     This class inherits SegmentationRunner and
     preprocesses the input type of the MPK
```

## deepview/validator/visualize/segmentationdrawer.py

```diff
@@ -85,99 +85,115 @@
         out_gt = out_gt.convert("RGB")
         dst = Image.new('RGB', (out_dt.width + out_gt.width, out_dt.height))
         dst.paste(out_dt, (0, 0))
         dst.paste(out_gt, (out_dt.width, 0))
         return dst
 
     @staticmethod
-    def mask2imagetransform(mask, rgba_colors, union=False):
+    def mask2imagetransform(mask, labels, union=False):
         """
         This method will transform a numpy array of mask into an RGBA image.
-        Current implementation will work only for two classes (person and car).
         Unit-test for this method is defined under:
             file: test/test_segmentationdrawer.py
             function: test_mask2imagetransform
 
         Parameter
         ---------
 
             mask: (height, width, 3) np.ndarray
                 Array representing the mask.
 
-            rgba_colors: tuple of ints
-                (R,G,B,A) color
+            labels: list
+                The list of prediction labels in the mask.
 
             union: bool
                 Specify to mask all objects with one color (True).
                 False otherwise.
 
         Returns
         -------
             image: PIL Image object
                 The mask image.
 
         Raises
         ------
             None
         """
+
+        colors = np.array([
+                [180, 0, 0],
+                [178, 179, 0],
+                [142, 206, 70],
+                [127, 96, 166],
+                [2, 1, 181],
+                [3, 152, 133],
+                [121, 121, 121],
+                [76, 0, 0],
+                [240, 0, 0],
+                [107, 123, 61],
+                [245, 185, 0],
+                [94, 78, 127],
+                [202, 2, 202],
+                [105, 153, 199],
+                [252, 155, 209],
+                [53, 76, 32],
+                [146, 76, 17],
+                [0, 166, 76],
+                [0, 219, 99],
+                [2, 71, 128]
+            ], np.uint8)
+
         # Transform dimension of masks from a 2D numpy array to 4D with RGBA
         # channels
         mask_4_channels = np.stack((mask,) * 4, axis=-1)
 
         if union:
-            # Assign both person and car with color white
+            # Assign all classes with color white
             mask_4_channels[mask_4_channels == 1] = 255
             # Temporarily unpack the bands for readability
             red, green, blue, _ = mask_4_channels.T
-            # Areas with both person and car
+            # Areas of all classes
             u_areas = (red == 255) & (blue == 255) & (green == 255)
-            # Color person and car with blue
-            mask_4_channels[..., :][u_areas.T] = rgba_colors[0]
+            # Color all classes with blue
+            mask_4_channels[..., :][u_areas.T] = (0, 0, 255, 130)
 
         else:
-            # Assign all person objects with color white (255, 255, 255)
-            mask_4_channels[mask_4_channels == 1] = 255
-            # Assign car objects with color grey ish (125, 125, 125)
-            mask_4_channels[mask_4_channels == 2] = 125
+            labels = np.sort(np.unique(labels))
+            for label in labels:
+                if label != 0:
+                    # Designate a color for each class
+                    mask_4_channels[mask_4_channels == label] = \
+                        colors[label][0]
+            
             # Temporarily unpack the bands for readability
             red, green, blue, _ = mask_4_channels.T
-
-            # Find areas with person objects ... (leaves alpha values alone...)
-            person_areas = (red == 255) & (blue == 255) & (green == 255)
-            # Find areas with car objects ... (leaves alpha values alone...)
-            car_areas = (red == 125) & (blue == 125) & (green == 125)
-
-            # Transpose back needed
-            # Color person objects with blue
-            mask_4_channels[..., :][person_areas.T] = rgba_colors[0]
-            # Color car objects with red
-            mask_4_channels[..., :][car_areas.T] = rgba_colors[1]
-
+            for label in labels:
+                if label != 0:
+                    # Find object areas ... (leaves alpha values alone...)
+                    object_areas = (red == colors[label][0]) &\
+                    (blue == colors[label][0]) & (green == colors[label][0])
+                    # Transpose back needed
+                    # Color person objects with blue
+                    mask_4_channels[..., :][object_areas.T] = \
+                        np.append(colors[label], 130)
+            
         # Convert array to image object for image processing
-        im2 = Image.fromarray(mask_4_channels.astype(np.uint8))
-
-        return im2
+        return Image.fromarray(mask_4_channels.astype(np.uint8))
 
-    def mask2maskimage(self, height, width, instances):
+    def mask2maskimage(self, instances):
         """
         This method masks the original image and returns the original image
         with mask prediction on the left and mask ground truth on the right.
         Unit-test for this method is defined under:
             file: test/test_segmentationdrawer.py
             function: test_mask2maskimage
 
         Parameters
         ----------
 
-            height: int
-                The height of the model input shape.
-
-            width: int
-                The width of the model input shape.
-
             instances: dict
                 This contains information such as:
 
                 .. code-block:: python
 
                     instances = {
                                 'gt_instance': {
@@ -203,38 +219,37 @@
 
         Raises
         ------
             None
         """
 
         font = ImageFont.load_default()
-        original_image = Image.open(instances.get(
-            'gt_instance').get('image_path')).convert('RGB').copy()
-        original_image = original_image.resize((width, height))
+        image = instances.get('gt_instance').get('image')
+        original_image = Image.fromarray(np.uint8(image))
 
         gt_mask = instances.get('gt_instance').get('gt_mask')
         dt_mask = instances.get('dt_instance').get('dt_mask')
+        gt_labels = instances.get('gt_instance').get('labels')
+        dt_labels = instances.get('dt_instance').get('labels')
 
         # Convert array to image object for image processing
         gt_im2 = self.mask2imagetransform(
-            gt_mask, rgba_colors=[
-                (0, 0, 255, 130), (255, 0, 0, 130)]
+            gt_mask, gt_labels
             )
         dt_im2 = self.mask2imagetransform(
-            dt_mask, rgba_colors=[
-                (0, 0, 255, 130), (255, 0, 0, 130)]
+            dt_mask, dt_labels
             )
 
         # convert img to RGBA mode
         image_gt = original_image.convert("RGBA")
         image_dt = original_image.convert("RGBA")
 
         out_gt = Image.alpha_composite(image_gt, gt_im2)
         out_gt = out_gt.convert("RGB")
-
+        
         out_dt = Image.alpha_composite(image_dt, dt_im2)
         out_dt = out_dt.convert("RGB")
 
         dst = Image.new('RGB', (out_dt.width + out_gt.width, out_dt.height))
         dst.paste(out_gt, (0, 0))
         dst.paste(out_dt, (out_dt.width, 0))
 
@@ -243,20 +258,19 @@
                       align='left', fill=(0, 0, 0))
         drawtext.text((out_dt.width, 0), "MODEL PREDICTION",
                       font=font, align='left', fill=(0, 0, 0))
         return dst
 
     def mask2mask4panes(
             self,
-            height,
-            width,
             instances,
             tp_mask,
             fp_mask,
-            fn_mask):
+            fn_mask
+        ):
         """
         This method creates a four pane image result:
 
             - The first pane shows the masks for ground truth.
             - Second pane shows masks for the model prediction.
             - Third pane shows masks for the union of ground truth and
               model prediction.
@@ -271,20 +285,14 @@
         Unit-test for this method is defined under:
             file: test/test_segmentationdrawer.py
             function: test_mask2mask4panes
 
         Parameters
         ----------
 
-            height: int
-                The height of the model input shape.
-
-            width: int
-                The width of the model input shape.
-
             instances: dict
                 This contains information such as:
 
                 .. code-block:: python
 
                     instances = {
                                 'gt_instance': {
@@ -320,43 +328,39 @@
 
         Raises
         ------
             None
         """
 
         font = ImageFont.load_default()
-        original_image = Image.open(instances.get(
-            'gt_instance').get('image_path')).convert('RGB').copy()
-        original_image = original_image.resize((width, height))
+        image = instances.get('gt_instance').get('image')
+        original_image = Image.fromarray(np.uint8(image))
 
         gt_mask = instances.get('gt_instance').get('gt_mask')
         dt_mask = instances.get('dt_instance').get('dt_mask')
+        gt_labels = instances.get('gt_instance').get('labels')
+        dt_labels = instances.get('dt_instance').get('labels')
 
         union_mask = np.add(dt_mask, gt_mask)
         union_mask = np.where(union_mask > 0, 1, union_mask)
 
         # Convert array to image object for image processing
         gt_im2 = self.mask2imagetransform(
-            gt_mask, rgba_colors=[
-                (0, 0, 255, 130), (255, 0, 0, 130)])
+            gt_mask, gt_labels)
         dt_im2 = self.mask2imagetransform(
-            dt_mask, rgba_colors=[
-                (0, 0, 255, 130), (255, 0, 0, 130)])
+            dt_mask, dt_labels)
         tp_im2 = self.mask2imagetransform(
-            tp_mask, rgba_colors=[
-                (0, 0, 255, 130), (255, 0, 0, 130)])
+            tp_mask, dt_labels)
         fp_im2 = self.mask2imagetransform(
-            fp_mask, rgba_colors=[
-                (137, 207, 240, 130), (255, 114, 118, 130)])
+            fp_mask, dt_labels)
         fn_im2 = self.mask2imagetransform(
-            fn_mask, rgba_colors=[
-                (137, 207, 240, 130), (255, 114, 118, 130)])
+            fn_mask, gt_labels)
         u_im2 = self.mask2imagetransform(
-            union_mask, rgba_colors=[
-                (0, 0, 255, 130)], union=True)
+            union_mask, gt_labels+dt_labels, 
+            union=True)
 
         # convert img to RGBA mode
         image_gt = original_image.convert("RGBA")
         image_dt = original_image.convert("RGBA")
         image_union = original_image.convert("RGBA")
         image_separate = original_image.convert("RGBA")
```

## Comparing `deepview_validator-3.0.3.dist-info/RECORD` & `deepview_validator-3.0.4.dist-info/RECORD`

 * *Files 7% similar despite different names*

```diff
@@ -1,44 +1,44 @@
 deepview/validator/__init__.py,sha256=OjgWWA-EkgVtRfV0-4yBzPOGUB7AaRD-ipzY5gmq1I4,760
-deepview/validator/__main__.py,sha256=oXd9h9VvZfCSWQ52aV_VSau75ZfvOzIICR85JOcEv1Y,15914
+deepview/validator/__main__.py,sha256=zFqN6mJ4PFhyb6WLrl4FEwzoqGwHWBfP3uZmebll4gk,16195
 deepview/validator/exceptions.py,sha256=wN9LfxtSZ7jijwua-jziTBsrynjPS5Y8-nZerxBe5m4,12847
 deepview/validator/datasets/__init__.py,sha256=kn3O2am9aQkpCCUapanHpD2Hmqg2IXhMMduOY6LizYY,497
 deepview/validator/datasets/core.py,sha256=Uo5fxKb_dX2eCUAfh-1B3PJD23y_kq4FO-FDPeJp64c,29989
-deepview/validator/datasets/darknet.py,sha256=Aypvp-JAUQL5rcm4Knny5AUfFfeNwExQbxU_3gdQcec,14505
+deepview/validator/datasets/darknet.py,sha256=RDq4im5sR029vqko1078qN05P7Aa9_rPu0MQYOld66s,14688
 deepview/validator/datasets/tfrecord.py,sha256=tuMWxHPSazhzLdKlZv7tPVZVIzevEp7snIWgpT9zuDI,7029
 deepview/validator/datasets/utils.py,sha256=uFo47noinkjsD_88e5Gj1nSFHvyqFY0guLE7I4LWPlU,3786
 deepview/validator/evaluators/__init__.py,sha256=0UF3SF__Siagxw9ERwngKeVVz8DlNCiqUd2I_u8SJvI,530
 deepview/validator/evaluators/core.py,sha256=tcMB8G7z_5qeXMD8hojl9Avl-nOsUm4IjMD1lVX1MCg,5067
 deepview/validator/evaluators/detectionevaluator.py,sha256=z85dWpX4oZ3ckMiU4jZ2Sg_fDJjOW9er75iGRlSLchY,24515
-deepview/validator/evaluators/segmentationevaluator.py,sha256=3hVpduQLVRKGhEhcNQT22OMUfNPPn2Y5kv3dTARyx4Y,18731
+deepview/validator/evaluators/segmentationevaluator.py,sha256=TeSQkXk1bloGuxQxo-Ik0OTWPPuwHbnijE49As2D1WA,19208
 deepview/validator/metrics/__init__.py,sha256=vp8Dy7OR0tzfFsBPv81Y2OYWetgWrAHRzfBzkkNtazY,830
-deepview/validator/metrics/core.py,sha256=zfqUyymrWuFhQo2VbxhR36cw6dTuI7Vk1z-pKKLFBgU,11297
+deepview/validator/metrics/core.py,sha256=Bz8SHzepcgVZN1kplxRURn8thZ_UA9mdulWJQIpoOWc,11367
 deepview/validator/metrics/detectiondata.py,sha256=Y5XeGWdaZjcYpOcvx67d2yOSTIynCBxKTVVdOAzQbEg,31668
 deepview/validator/metrics/detectionmetrics.py,sha256=JwkVH8tvbz055ETPyChi4mcYlDu0XPs4XBUk0pE5ipM,25023
 deepview/validator/metrics/detectionutils.py,sha256=4LaHrYYw9TM8C_6pWxnfOTSz2mjHMUCNnBKqfFD6NAk,4548
-deepview/validator/metrics/segmentationdata.py,sha256=pW1KijcEyhWwIP-DOsEnQe0ACXJeOIezSRneCjqlglE,17211
-deepview/validator/metrics/segmentationmetrics.py,sha256=SwydgF85ql4nO_jUgPZeIV6Y9rwIS9_0POPu4CTV_Og,6319
-deepview/validator/metrics/segmentationutils.py,sha256=bbD5xAz-xCLcoaRXFHDYW7Zc_L8oqyVJ2ccItC5YtCc,11704
-deepview/validator/runners/__init__.py,sha256=RpTNgMnmfh7LrQwMZk8RJOawR8lPBP1tpY8wt8sO-Ao,674
+deepview/validator/metrics/segmentationdata.py,sha256=28ws5aceUvg8jzNfGWtfKRp4EvFYwYsw6ddpj_SIjv4,17192
+deepview/validator/metrics/segmentationmetrics.py,sha256=Fe68fqugRkYZDTjXw5T_NbsrapKByMkZRXfvVueZXCU,6514
+deepview/validator/metrics/segmentationutils.py,sha256=HdI0xTKKO7GAfil42EjpI3_YAefKHCNDjJFD0wnt9UI,11690
+deepview/validator/runners/__init__.py,sha256=gRRd8B9lhTPg6noEF-nRipyIsawSJSp6BapSQzxDXjc,735
 deepview/validator/runners/core.py,sha256=gVikDHobZPPeQxvK1eRzaoKgert1RlJQQBHpz-3zBg4,5144
-deepview/validator/runners/deepviewrt.py,sha256=KgJhzm_HpQuydzjiI7TKt6wUo2k-rodQcv49wj-p_eE,13596
-deepview/validator/runners/keras.py,sha256=bc4arQLR0WPLjStqQek0Xe_Xxg6pzQHmS6hAG3SBotY,14335
+deepview/validator/runners/deepviewrt.py,sha256=80JsM3F4tbWReeAi0r_PBpONrpuk107XfSGPZsKtr-I,13520
+deepview/validator/runners/keras.py,sha256=e1FOHMiY5AikhBDSbMTfKiyqknQU3XuG5feMjaYxJYA,17082
 deepview/validator/runners/offline.py,sha256=DTcgg7LmuUXUuUmFw5qEo0IAYjjgaVaG0lpNCKAzqbM,6147
 deepview/validator/runners/tensorrt.py,sha256=OMBziiYWUeJWmjonHo5QlQZMXSDppHAOx0CDJFEZAfY,16285
-deepview/validator/runners/tflite.py,sha256=yJst2M2ae2ID003msEKu03gpKgxykRWjzGMFssffM2A,15389
-deepview/validator/runners/modelclient/__init__.py,sha256=xAXPMiSEFRfLGKbT7uCiutSxvzWkVrdN9HNx2XUVMKI,600
-deepview/validator/runners/modelclient/boxes.py,sha256=_j7gKuS7Vw0iYT6b8ocmX4qiPzc2HrH17_TgkM89D2c,29387
+deepview/validator/runners/tflite.py,sha256=iecNtbftpzoxMRJ41xqx4QZIFuIsIK76eY8gA9Gpnk4,14493
+deepview/validator/runners/modelclient/__init__.py,sha256=5S2qsbvi8d9y8-FH6Zu7VZS8WlZZKwRg7yhFd58t3uw,612
+deepview/validator/runners/modelclient/boxes.py,sha256=FRGG_EwcyXLZq-CKqLen00HcM0M770d1m35UnfRXIkc,27984
 deepview/validator/runners/modelclient/core.py,sha256=1zK1SxMojNnEk6-BT8tRdyMroad2Bvg9vs5vu9-dNTw,3116
-deepview/validator/runners/modelclient/segmentation.py,sha256=7n8Xf7ZB5c3tL2q87QLCkBJlETq2r1wNLdN33DG2ZuI,11846
+deepview/validator/runners/modelclient/segmentation.py,sha256=TUGnwLy6Pih2Alp9W20_f4usTObLGRNJ8RKlp9Xa8x0,11195
 deepview/validator/visualize/__init__.py,sha256=gGM_U0YR1SOagPIhUjQr8PLJAPZ-XzHuY9jLwg3uHds,521
 deepview/validator/visualize/core.py,sha256=qELc2h8wID5ROWiVUeLFZkgBUzO8KCQiQkJUqR_ZFgc,11059
 deepview/validator/visualize/detectiondrawer.py,sha256=3CMifGWwfx4cuKNwE8WLgmEGRIBaFewfTjmknlpPJeg,7014
-deepview/validator/visualize/segmentationdrawer.py,sha256=Ub7TixeNRleaDcN4c3wXHQ0xtUDuXpvd4cABsxfKB8o,14479
+deepview/validator/visualize/segmentationdrawer.py,sha256=WYMzIPbqcFy-8_RGrs_TTdqjvI-5Q8A08rQHN5Ymp1w,14370
 deepview/validator/writers/__init__.py,sha256=PAF-P5kc2h-0VYIz6KL5Z4jgzpKcu4ulIeDsOldKTjE,574
 deepview/validator/writers/console.py,sha256=oIDnZdDtlc0snO24ODN_-54vrdWrvUB0J3AI08RCtuE,7611
 deepview/validator/writers/core.py,sha256=zSZ58ByNrJbTyJqDb8Tohw-SvzJj3CoAqoyf963vfG4,19839
 deepview/validator/writers/tensorboard.py,sha256=NxzZolLWfAlz1G8gyOZxYRlYmzM0F2KijPrHkKvA8nE,9065
-deepview_validator-3.0.3.dist-info/METADATA,sha256=HTpgtqA-b28cG2mK9rVrIjpVATxwrTVwTA6hE0BsluM,433
-deepview_validator-3.0.3.dist-info/WHEEL,sha256=nvhOrkn7_9sGzJjxuUFjoJ6OkO7SJJqHSjq9VNu0Elc,92
-deepview_validator-3.0.3.dist-info/entry_points.txt,sha256=n4jIdEDC_mPGVLwmS21vEFC8_D7mqNuekZYdtupSSVE,73
-deepview_validator-3.0.3.dist-info/top_level.txt,sha256=FZ_uj5ZExs9dTNq5lw196yb-XR3VHKi6vS0EWgTQtXk,9
-deepview_validator-3.0.3.dist-info/RECORD,,
+deepview_validator-3.0.4.dist-info/METADATA,sha256=AI09Bbnwz1dji3g6iYgqEugWBqBJQBkEJqKKf9_TdTY,468
+deepview_validator-3.0.4.dist-info/WHEEL,sha256=nvhOrkn7_9sGzJjxuUFjoJ6OkO7SJJqHSjq9VNu0Elc,92
+deepview_validator-3.0.4.dist-info/entry_points.txt,sha256=n4jIdEDC_mPGVLwmS21vEFC8_D7mqNuekZYdtupSSVE,73
+deepview_validator-3.0.4.dist-info/top_level.txt,sha256=FZ_uj5ZExs9dTNq5lw196yb-XR3VHKi6vS0EWgTQtXk,9
+deepview_validator-3.0.4.dist-info/RECORD,,
```

