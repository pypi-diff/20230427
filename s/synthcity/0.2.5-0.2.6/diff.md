# Comparing `tmp/synthcity-0.2.5-py3-none-macosx_10_14_x86_64.whl.zip` & `tmp/synthcity-0.2.6-py3-none-macosx_10_14_x86_64.whl.zip`

## zipinfo {}

```diff
@@ -1,152 +1,156 @@
-Zip file size: 343002 bytes, number of entries: 150
--rw-r--r--  2.0 unx      627 b- defN 23-Apr-04 12:08 synthcity/__init__.py
--rw-r--r--  2.0 unx     2765 b- defN 23-Apr-04 12:08 synthcity/logger.py
--rw-r--r--  2.0 unx      120 b- defN 23-Apr-04 12:08 synthcity/version.py
--rw-r--r--  2.0 unx    15727 b- defN 23-Apr-04 12:08 synthcity/benchmark/__init__.py
--rw-r--r--  2.0 unx     8853 b- defN 23-Apr-04 12:08 synthcity/benchmark/utils.py
--rw-r--r--  2.0 unx      121 b- defN 23-Apr-04 12:08 synthcity/metrics/__init__.py
--rw-r--r--  2.0 unx     3757 b- defN 23-Apr-04 12:08 synthcity/metrics/_utils.py
--rw-r--r--  2.0 unx     7885 b- defN 23-Apr-04 12:08 synthcity/metrics/eval.py
--rw-r--r--  2.0 unx     6118 b- defN 23-Apr-04 12:08 synthcity/metrics/eval_attacks.py
--rw-r--r--  2.0 unx    10365 b- defN 23-Apr-04 12:08 synthcity/metrics/eval_detection.py
--rw-r--r--  2.0 unx    39838 b- defN 23-Apr-04 12:08 synthcity/metrics/eval_performance.py
--rw-r--r--  2.0 unx    12244 b- defN 23-Apr-04 12:08 synthcity/metrics/eval_privacy.py
--rw-r--r--  2.0 unx     7700 b- defN 23-Apr-04 12:08 synthcity/metrics/eval_sanity.py
--rw-r--r--  2.0 unx    29713 b- defN 23-Apr-04 12:08 synthcity/metrics/eval_statistical.py
--rw-r--r--  2.0 unx     3071 b- defN 23-Apr-04 12:08 synthcity/metrics/plots.py
--rw-r--r--  2.0 unx     4296 b- defN 23-Apr-04 12:08 synthcity/metrics/scores.py
--rw-r--r--  2.0 unx     3587 b- defN 23-Apr-04 12:08 synthcity/metrics/weighted_metrics.py
--rw-r--r--  2.0 unx       71 b- defN 23-Apr-04 12:08 synthcity/metrics/core/__init__.py
--rw-r--r--  2.0 unx     4397 b- defN 23-Apr-04 12:08 synthcity/metrics/core/metric.py
--rw-r--r--  2.0 unx     5674 b- defN 23-Apr-04 12:08 synthcity/metrics/representations/OneClass.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-04 12:08 synthcity/metrics/representations/__init__.py
--rw-r--r--  2.0 unx     2164 b- defN 23-Apr-04 12:08 synthcity/metrics/representations/networks.py
--rw-r--r--  2.0 unx      892 b- defN 23-Apr-04 12:08 synthcity/plugins/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-04 12:08 synthcity/plugins/core/__init__.py
--rw-r--r--  2.0 unx    10277 b- defN 23-Apr-04 12:08 synthcity/plugins/core/constraints.py
--rw-r--r--  2.0 unx    60621 b- defN 23-Apr-04 12:08 synthcity/plugins/core/dataloader.py
--rw-r--r--  2.0 unx     4995 b- defN 23-Apr-04 12:08 synthcity/plugins/core/dataset.py
--rw-r--r--  2.0 unx    13163 b- defN 23-Apr-04 12:08 synthcity/plugins/core/distribution.py
--rw-r--r--  2.0 unx    25106 b- defN 23-Apr-04 12:08 synthcity/plugins/core/plugin.py
--rw-r--r--  2.0 unx     7594 b- defN 23-Apr-04 12:08 synthcity/plugins/core/schema.py
--rw-r--r--  2.0 unx     4417 b- defN 23-Apr-04 12:08 synthcity/plugins/core/serializable.py
--rw-r--r--  2.0 unx    11889 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/RGCNConv.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/__init__.py
--rw-r--r--  2.0 unx    19210 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/convnet.py
--rw-r--r--  2.0 unx     3666 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/data_encoder.py
--rw-r--r--  2.0 unx    16293 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/flows.py
--rw-r--r--  2.0 unx    27155 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/gan.py
--rw-r--r--  2.0 unx    25132 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/goggle.py
--rw-r--r--  2.0 unx    25246 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/image_gan.py
--rw-r--r--  2.0 unx     1008 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/layers.py
--rw-r--r--  2.0 unx    15212 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/mlp.py
--rw-r--r--  2.0 unx    25471 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/tabular_encoder.py
--rw-r--r--  2.0 unx     6440 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/tabular_flows.py
--rw-r--r--  2.0 unx    18358 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/tabular_gan.py
--rw-r--r--  2.0 unx     9703 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/tabular_goggle.py
--rw-r--r--  2.0 unx    11061 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/tabular_vae.py
--rw-r--r--  2.0 unx     2051 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/transformer.py
--rw-r--r--  2.0 unx    31108 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/ts_gan.py
--rw-r--r--  2.0 unx    26260 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/ts_model.py
--rw-r--r--  2.0 unx    12754 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/ts_tabular_gan.py
--rw-r--r--  2.0 unx     9637 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/ts_tabular_vae.py
--rw-r--r--  2.0 unx    21111 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/ts_vae.py
--rw-r--r--  2.0 unx    20720 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/vae.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/dag/__init__.py
--rw-r--r--  2.0 unx     2083 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/dag/data.py
--rw-r--r--  2.0 unx     7343 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/dag/dsl.py
--rw-r--r--  2.0 unx     7926 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/dag/dstruct.py
--rw-r--r--  2.0 unx     9253 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/dag/utils.py
--rw-r--r--  2.0 unx      389 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/survival_analysis/__init__.py
--rw-r--r--  2.0 unx     1493 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/survival_analysis/_base.py
--rw-r--r--  2.0 unx     8280 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/survival_analysis/benchmarks.py
--rw-r--r--  2.0 unx     1987 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/survival_analysis/loader.py
--rw-r--r--  2.0 unx     4487 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/survival_analysis/metrics.py
--rw-r--r--  2.0 unx     2223 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/survival_analysis/surv_aft.py
--rw-r--r--  2.0 unx     2597 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/survival_analysis/surv_coxph.py
--rw-r--r--  2.0 unx     5214 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/survival_analysis/surv_deephit.py
--rw-r--r--  2.0 unx     5954 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/survival_analysis/surv_xgb.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/survival_analysis/third_party/__init__.py
--rw-r--r--  2.0 unx    14800 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/survival_analysis/third_party/metrics.py
--rw-r--r--  2.0 unx    11564 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/survival_analysis/third_party/nonparametric.py
--rw-r--r--  2.0 unx     9513 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/survival_analysis/third_party/util.py
--rw-r--r--  2.0 unx     7380 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/tabular_ddpm/__init__.py
--rw-r--r--  2.0 unx    34804 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/tabular_ddpm/gaussian_multinomial_diffsuion.py
--rw-r--r--  2.0 unx     3424 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/tabular_ddpm/modules.py
--rw-r--r--  2.0 unx     5250 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/tabular_ddpm/utils.py
--rw-r--r--  2.0 unx      432 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/time_series_survival/__init__.py
--rw-r--r--  2.0 unx     1738 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/time_series_survival/_base.py
--rw-r--r--  2.0 unx    12847 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/time_series_survival/benchmarks.py
--rw-r--r--  2.0 unx      552 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/time_series_survival/loader.py
--rw-r--r--  2.0 unx     3578 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/time_series_survival/ts_surv_coxph.py
--rw-r--r--  2.0 unx    25693 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/time_series_survival/ts_surv_dynamic_deephit.py
--rw-r--r--  2.0 unx     4357 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/time_series_survival/ts_surv_xgb.py
--rw-r--r--  2.0 unx     4644 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/time_series_survival/utils.py
--rw-r--r--  2.0 unx      204 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/time_to_event/__init__.py
--rw-r--r--  2.0 unx     2294 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/time_to_event/_base.py
--rw-r--r--  2.0 unx     6069 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/time_to_event/benchmarks.py
--rw-r--r--  2.0 unx      843 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/time_to_event/loader.py
--rw-r--r--  2.0 unx     3656 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/time_to_event/metrics.py
--rw-r--r--  2.0 unx     2079 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/time_to_event/tte_aft.py
--rw-r--r--  2.0 unx     2058 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/time_to_event/tte_coxph.py
--rw-r--r--  2.0 unx    15786 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/time_to_event/tte_date.py
--rw-r--r--  2.0 unx     4913 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/time_to_event/tte_deephit.py
--rw-r--r--  2.0 unx     2920 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/time_to_event/tte_survival_function_regression.py
--rw-r--r--  2.0 unx     4859 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/time_to_event/tte_survival_time_series.py
--rw-r--r--  2.0 unx    14973 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/time_to_event/tte_tenn.py
--rw-r--r--  2.0 unx     4816 b- defN 23-Apr-04 12:08 synthcity/plugins/core/models/time_to_event/tte_xgb.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-04 12:08 synthcity/plugins/domain_adaptation/__init__.py
--rw-r--r--  2.0 unx    34278 b- defN 23-Apr-04 12:08 synthcity/plugins/domain_adaptation/plugin_radialgan.py
--rw-r--r--  2.0 unx      451 b- defN 23-Apr-04 12:08 synthcity/plugins/generic/__init__.py
--rw-r--r--  2.0 unx     6830 b- defN 23-Apr-04 12:08 synthcity/plugins/generic/plugin_bayesian_network.py
--rw-r--r--  2.0 unx    11636 b- defN 23-Apr-04 12:08 synthcity/plugins/generic/plugin_ctgan.py
--rw-r--r--  2.0 unx     8814 b- defN 23-Apr-04 12:08 synthcity/plugins/generic/plugin_ddpm.py
--rw-r--r--  2.0 unx     1964 b- defN 23-Apr-04 12:08 synthcity/plugins/generic/plugin_dummy_sampler.py
--rw-r--r--  2.0 unx    10298 b- defN 23-Apr-04 12:08 synthcity/plugins/generic/plugin_goggle.py
--rw-r--r--  2.0 unx     2028 b- defN 23-Apr-04 12:08 synthcity/plugins/generic/plugin_marginal_distributions.py
--rw-r--r--  2.0 unx    11759 b- defN 23-Apr-04 12:08 synthcity/plugins/generic/plugin_nflow.py
--rw-r--r--  2.0 unx     8920 b- defN 23-Apr-04 12:08 synthcity/plugins/generic/plugin_rtvae.py
--rw-r--r--  2.0 unx     9041 b- defN 23-Apr-04 12:08 synthcity/plugins/generic/plugin_tvae.py
--rw-r--r--  2.0 unx     1943 b- defN 23-Apr-04 12:08 synthcity/plugins/generic/plugin_uniform_sampler.py
--rw-r--r--  2.0 unx      447 b- defN 23-Apr-04 12:08 synthcity/plugins/images/__init__.py
--rw-r--r--  2.0 unx    13100 b- defN 23-Apr-04 12:08 synthcity/plugins/images/plugin_image_adsgan.py
--rw-r--r--  2.0 unx    12595 b- defN 23-Apr-04 12:08 synthcity/plugins/images/plugin_image_cgan.py
--rw-r--r--  2.0 unx      451 b- defN 23-Apr-04 12:08 synthcity/plugins/privacy/__init__.py
--rw-r--r--  2.0 unx    11937 b- defN 23-Apr-04 12:08 synthcity/plugins/privacy/plugin_adsgan.py
--rw-r--r--  2.0 unx    17470 b- defN 23-Apr-04 12:08 synthcity/plugins/privacy/plugin_decaf.py
--rw-r--r--  2.0 unx    12297 b- defN 23-Apr-04 12:08 synthcity/plugins/privacy/plugin_dpgan.py
--rw-r--r--  2.0 unx    19601 b- defN 23-Apr-04 12:08 synthcity/plugins/privacy/plugin_pategan.py
--rw-r--r--  2.0 unx    22950 b- defN 23-Apr-04 12:08 synthcity/plugins/privacy/plugin_privbayes.py
--rw-r--r--  2.0 unx      490 b- defN 23-Apr-04 12:08 synthcity/plugins/survival_analysis/__init__.py
--rw-r--r--  2.0 unx     7534 b- defN 23-Apr-04 12:08 synthcity/plugins/survival_analysis/_survival_pipeline.py
--rw-r--r--  2.0 unx     5873 b- defN 23-Apr-04 12:08 synthcity/plugins/survival_analysis/plugin_survae.py
--rw-r--r--  2.0 unx     5943 b- defN 23-Apr-04 12:08 synthcity/plugins/survival_analysis/plugin_survival_ctgan.py
--rw-r--r--  2.0 unx     8082 b- defN 23-Apr-04 12:08 synthcity/plugins/survival_analysis/plugin_survival_gan.py
--rw-r--r--  2.0 unx     5626 b- defN 23-Apr-04 12:08 synthcity/plugins/survival_analysis/plugin_survival_nflow.py
--rw-r--r--  2.0 unx      472 b- defN 23-Apr-04 12:08 synthcity/plugins/time_series/__init__.py
--rw-r--r--  2.0 unx    10607 b- defN 23-Apr-04 12:08 synthcity/plugins/time_series/plugin_fflows.py
--rw-r--r--  2.0 unx    18792 b- defN 23-Apr-04 12:08 synthcity/plugins/time_series/plugin_timegan.py
--rw-r--r--  2.0 unx    13933 b- defN 23-Apr-04 12:08 synthcity/plugins/time_series/plugin_timevae.py
--rw-r--r--  2.0 unx    12146 b- defN 23-Apr-04 12:08 synthcity/utils/anonymization.py
--rw-r--r--  2.0 unx     2691 b- defN 23-Apr-04 12:08 synthcity/utils/callbacks.py
--rw-r--r--  2.0 unx     5705 b- defN 23-Apr-04 12:08 synthcity/utils/compression.py
--rw-r--r--  2.0 unx       98 b- defN 23-Apr-04 12:08 synthcity/utils/constants.py
--rw-r--r--  2.0 unx      575 b- defN 23-Apr-04 12:08 synthcity/utils/dataframe.py
--rw-r--r--  2.0 unx     2775 b- defN 23-Apr-04 12:08 synthcity/utils/evaluation.py
--rw-r--r--  2.0 unx     6482 b- defN 23-Apr-04 12:08 synthcity/utils/optimizer.py
--rw-r--r--  2.0 unx      653 b- defN 23-Apr-04 12:08 synthcity/utils/redis_wrapper.py
--rw-r--r--  2.0 unx      473 b- defN 23-Apr-04 12:08 synthcity/utils/reproducibility.py
--rw-r--r--  2.0 unx    11212 b- defN 23-Apr-04 12:08 synthcity/utils/samplers.py
--rw-r--r--  2.0 unx     1062 b- defN 23-Apr-04 12:08 synthcity/utils/serialization.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-04 12:08 synthcity/utils/datasets/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-04 12:08 synthcity/utils/datasets/time_series/__init__.py
--rw-r--r--  2.0 unx     2617 b- defN 23-Apr-04 12:08 synthcity/utils/datasets/time_series/google_stocks.py
--rw-r--r--  2.0 unx     5973 b- defN 23-Apr-04 12:08 synthcity/utils/datasets/time_series/pbc.py
--rw-r--r--  2.0 unx     2722 b- defN 23-Apr-04 12:08 synthcity/utils/datasets/time_series/sine.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-04 12:08 synthcity/utils/datasets/time_series/data/__init__.py
--rw-r--r--  2.0 unx    11357 b- defN 23-Apr-04 12:08 synthcity-0.2.5.dist-info/LICENSE
--rw-r--r--  2.0 unx    28920 b- defN 23-Apr-04 12:08 synthcity-0.2.5.dist-info/METADATA
--rw-r--r--  2.0 unx      108 b- defN 23-Apr-04 12:08 synthcity-0.2.5.dist-info/WHEEL
--rw-r--r--  2.0 unx       10 b- defN 23-Apr-04 12:08 synthcity-0.2.5.dist-info/top_level.txt
--rw-rw-r--  2.0 unx    15209 b- defN 23-Apr-04 12:08 synthcity-0.2.5.dist-info/RECORD
-150 files, 1307868 bytes uncompressed, 318150 bytes compressed:  75.7%
+Zip file size: 352429 bytes, number of entries: 154
+-rw-r--r--  2.0 unx      627 b- defN 23-Apr-27 08:53 synthcity/__init__.py
+-rw-r--r--  2.0 unx     2765 b- defN 23-Apr-27 08:53 synthcity/logger.py
+-rw-r--r--  2.0 unx      120 b- defN 23-Apr-27 08:53 synthcity/version.py
+-rw-r--r--  2.0 unx    15727 b- defN 23-Apr-27 08:53 synthcity/benchmark/__init__.py
+-rw-r--r--  2.0 unx     8853 b- defN 23-Apr-27 08:53 synthcity/benchmark/utils.py
+-rw-r--r--  2.0 unx      121 b- defN 23-Apr-27 08:53 synthcity/metrics/__init__.py
+-rw-r--r--  2.0 unx     3757 b- defN 23-Apr-27 08:53 synthcity/metrics/_utils.py
+-rw-r--r--  2.0 unx     7885 b- defN 23-Apr-27 08:53 synthcity/metrics/eval.py
+-rw-r--r--  2.0 unx     6118 b- defN 23-Apr-27 08:53 synthcity/metrics/eval_attacks.py
+-rw-r--r--  2.0 unx    10365 b- defN 23-Apr-27 08:53 synthcity/metrics/eval_detection.py
+-rw-r--r--  2.0 unx    39838 b- defN 23-Apr-27 08:53 synthcity/metrics/eval_performance.py
+-rw-r--r--  2.0 unx    12244 b- defN 23-Apr-27 08:53 synthcity/metrics/eval_privacy.py
+-rw-r--r--  2.0 unx     7700 b- defN 23-Apr-27 08:53 synthcity/metrics/eval_sanity.py
+-rw-r--r--  2.0 unx    29713 b- defN 23-Apr-27 08:53 synthcity/metrics/eval_statistical.py
+-rw-r--r--  2.0 unx     3071 b- defN 23-Apr-27 08:53 synthcity/metrics/plots.py
+-rw-r--r--  2.0 unx     4296 b- defN 23-Apr-27 08:53 synthcity/metrics/scores.py
+-rw-r--r--  2.0 unx     3587 b- defN 23-Apr-27 08:53 synthcity/metrics/weighted_metrics.py
+-rw-r--r--  2.0 unx       71 b- defN 23-Apr-27 08:53 synthcity/metrics/core/__init__.py
+-rw-r--r--  2.0 unx     4397 b- defN 23-Apr-27 08:53 synthcity/metrics/core/metric.py
+-rw-r--r--  2.0 unx     5674 b- defN 23-Apr-27 08:53 synthcity/metrics/representations/OneClass.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-27 08:53 synthcity/metrics/representations/__init__.py
+-rw-r--r--  2.0 unx     2164 b- defN 23-Apr-27 08:53 synthcity/metrics/representations/networks.py
+-rw-r--r--  2.0 unx      892 b- defN 23-Apr-27 08:53 synthcity/plugins/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-27 08:53 synthcity/plugins/core/__init__.py
+-rw-r--r--  2.0 unx    10277 b- defN 23-Apr-27 08:53 synthcity/plugins/core/constraints.py
+-rw-r--r--  2.0 unx    60624 b- defN 23-Apr-27 08:53 synthcity/plugins/core/dataloader.py
+-rw-r--r--  2.0 unx     4995 b- defN 23-Apr-27 08:53 synthcity/plugins/core/dataset.py
+-rw-r--r--  2.0 unx    13845 b- defN 23-Apr-27 08:53 synthcity/plugins/core/distribution.py
+-rw-r--r--  2.0 unx    25106 b- defN 23-Apr-27 08:53 synthcity/plugins/core/plugin.py
+-rw-r--r--  2.0 unx     7608 b- defN 23-Apr-27 08:53 synthcity/plugins/core/schema.py
+-rw-r--r--  2.0 unx     4417 b- defN 23-Apr-27 08:53 synthcity/plugins/core/serializable.py
+-rw-r--r--  2.0 unx    11889 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/RGCNConv.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/__init__.py
+-rw-r--r--  2.0 unx    19210 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/convnet.py
+-rw-r--r--  2.0 unx     3848 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/factory.py
+-rw-r--r--  2.0 unx     9270 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/feature_encoder.py
+-rw-r--r--  2.0 unx    16293 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/flows.py
+-rw-r--r--  2.0 unx     4884 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/functions.py
+-rw-r--r--  2.0 unx    27155 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/gan.py
+-rw-r--r--  2.0 unx    25119 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/goggle.py
+-rw-r--r--  2.0 unx    25246 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/image_gan.py
+-rw-r--r--  2.0 unx     5209 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/layers.py
+-rw-r--r--  2.0 unx    11431 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/mlp.py
+-rw-r--r--  2.0 unx    17756 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/tabnet.py
+-rw-r--r--  2.0 unx    22043 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/tabular_encoder.py
+-rw-r--r--  2.0 unx     6440 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/tabular_flows.py
+-rw-r--r--  2.0 unx    18358 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/tabular_gan.py
+-rw-r--r--  2.0 unx     9703 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/tabular_goggle.py
+-rw-r--r--  2.0 unx    11061 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/tabular_vae.py
+-rw-r--r--  2.0 unx     2051 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/transformer.py
+-rw-r--r--  2.0 unx    31108 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/ts_gan.py
+-rw-r--r--  2.0 unx    26357 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/ts_model.py
+-rw-r--r--  2.0 unx    12754 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/ts_tabular_gan.py
+-rw-r--r--  2.0 unx     9637 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/ts_tabular_vae.py
+-rw-r--r--  2.0 unx    21111 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/ts_vae.py
+-rw-r--r--  2.0 unx    20720 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/vae.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/dag/__init__.py
+-rw-r--r--  2.0 unx     2083 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/dag/data.py
+-rw-r--r--  2.0 unx     7343 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/dag/dsl.py
+-rw-r--r--  2.0 unx     7926 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/dag/dstruct.py
+-rw-r--r--  2.0 unx     9253 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/dag/utils.py
+-rw-r--r--  2.0 unx      389 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/survival_analysis/__init__.py
+-rw-r--r--  2.0 unx     1493 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/survival_analysis/_base.py
+-rw-r--r--  2.0 unx     8280 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/survival_analysis/benchmarks.py
+-rw-r--r--  2.0 unx     1987 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/survival_analysis/loader.py
+-rw-r--r--  2.0 unx     4487 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/survival_analysis/metrics.py
+-rw-r--r--  2.0 unx     2223 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/survival_analysis/surv_aft.py
+-rw-r--r--  2.0 unx     2597 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/survival_analysis/surv_coxph.py
+-rw-r--r--  2.0 unx     5214 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/survival_analysis/surv_deephit.py
+-rw-r--r--  2.0 unx     5954 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/survival_analysis/surv_xgb.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/survival_analysis/third_party/__init__.py
+-rw-r--r--  2.0 unx    14800 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/survival_analysis/third_party/metrics.py
+-rw-r--r--  2.0 unx    11564 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/survival_analysis/third_party/nonparametric.py
+-rw-r--r--  2.0 unx     9513 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/survival_analysis/third_party/util.py
+-rw-r--r--  2.0 unx     7249 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/tabular_ddpm/__init__.py
+-rw-r--r--  2.0 unx    34437 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/tabular_ddpm/gaussian_multinomial_diffsuion.py
+-rw-r--r--  2.0 unx     3683 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/tabular_ddpm/modules.py
+-rw-r--r--  2.0 unx     5250 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/tabular_ddpm/utils.py
+-rw-r--r--  2.0 unx      432 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/time_series_survival/__init__.py
+-rw-r--r--  2.0 unx     1738 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/time_series_survival/_base.py
+-rw-r--r--  2.0 unx    12847 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/time_series_survival/benchmarks.py
+-rw-r--r--  2.0 unx      552 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/time_series_survival/loader.py
+-rw-r--r--  2.0 unx     3578 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/time_series_survival/ts_surv_coxph.py
+-rw-r--r--  2.0 unx    25693 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/time_series_survival/ts_surv_dynamic_deephit.py
+-rw-r--r--  2.0 unx     4357 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/time_series_survival/ts_surv_xgb.py
+-rw-r--r--  2.0 unx     4644 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/time_series_survival/utils.py
+-rw-r--r--  2.0 unx      204 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/time_to_event/__init__.py
+-rw-r--r--  2.0 unx     2294 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/time_to_event/_base.py
+-rw-r--r--  2.0 unx     6069 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/time_to_event/benchmarks.py
+-rw-r--r--  2.0 unx      843 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/time_to_event/loader.py
+-rw-r--r--  2.0 unx     3656 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/time_to_event/metrics.py
+-rw-r--r--  2.0 unx     2079 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/time_to_event/tte_aft.py
+-rw-r--r--  2.0 unx     2058 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/time_to_event/tte_coxph.py
+-rw-r--r--  2.0 unx    15786 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/time_to_event/tte_date.py
+-rw-r--r--  2.0 unx     4913 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/time_to_event/tte_deephit.py
+-rw-r--r--  2.0 unx     2920 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/time_to_event/tte_survival_function_regression.py
+-rw-r--r--  2.0 unx     4859 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/time_to_event/tte_survival_time_series.py
+-rw-r--r--  2.0 unx    14973 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/time_to_event/tte_tenn.py
+-rw-r--r--  2.0 unx     4816 b- defN 23-Apr-27 08:53 synthcity/plugins/core/models/time_to_event/tte_xgb.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-27 08:53 synthcity/plugins/domain_adaptation/__init__.py
+-rw-r--r--  2.0 unx    34278 b- defN 23-Apr-27 08:53 synthcity/plugins/domain_adaptation/plugin_radialgan.py
+-rw-r--r--  2.0 unx      451 b- defN 23-Apr-27 08:53 synthcity/plugins/generic/__init__.py
+-rw-r--r--  2.0 unx     6830 b- defN 23-Apr-27 08:53 synthcity/plugins/generic/plugin_bayesian_network.py
+-rw-r--r--  2.0 unx    11636 b- defN 23-Apr-27 08:53 synthcity/plugins/generic/plugin_ctgan.py
+-rw-r--r--  2.0 unx     9120 b- defN 23-Apr-27 08:53 synthcity/plugins/generic/plugin_ddpm.py
+-rw-r--r--  2.0 unx     1964 b- defN 23-Apr-27 08:53 synthcity/plugins/generic/plugin_dummy_sampler.py
+-rw-r--r--  2.0 unx    10298 b- defN 23-Apr-27 08:53 synthcity/plugins/generic/plugin_goggle.py
+-rw-r--r--  2.0 unx     2028 b- defN 23-Apr-27 08:53 synthcity/plugins/generic/plugin_marginal_distributions.py
+-rw-r--r--  2.0 unx    11759 b- defN 23-Apr-27 08:53 synthcity/plugins/generic/plugin_nflow.py
+-rw-r--r--  2.0 unx     8920 b- defN 23-Apr-27 08:53 synthcity/plugins/generic/plugin_rtvae.py
+-rw-r--r--  2.0 unx     9041 b- defN 23-Apr-27 08:53 synthcity/plugins/generic/plugin_tvae.py
+-rw-r--r--  2.0 unx     1943 b- defN 23-Apr-27 08:53 synthcity/plugins/generic/plugin_uniform_sampler.py
+-rw-r--r--  2.0 unx      447 b- defN 23-Apr-27 08:53 synthcity/plugins/images/__init__.py
+-rw-r--r--  2.0 unx    13100 b- defN 23-Apr-27 08:53 synthcity/plugins/images/plugin_image_adsgan.py
+-rw-r--r--  2.0 unx    12595 b- defN 23-Apr-27 08:53 synthcity/plugins/images/plugin_image_cgan.py
+-rw-r--r--  2.0 unx      451 b- defN 23-Apr-27 08:53 synthcity/plugins/privacy/__init__.py
+-rw-r--r--  2.0 unx    11937 b- defN 23-Apr-27 08:53 synthcity/plugins/privacy/plugin_adsgan.py
+-rw-r--r--  2.0 unx    17470 b- defN 23-Apr-27 08:53 synthcity/plugins/privacy/plugin_decaf.py
+-rw-r--r--  2.0 unx    12297 b- defN 23-Apr-27 08:53 synthcity/plugins/privacy/plugin_dpgan.py
+-rw-r--r--  2.0 unx    19601 b- defN 23-Apr-27 08:53 synthcity/plugins/privacy/plugin_pategan.py
+-rw-r--r--  2.0 unx    22950 b- defN 23-Apr-27 08:53 synthcity/plugins/privacy/plugin_privbayes.py
+-rw-r--r--  2.0 unx      490 b- defN 23-Apr-27 08:53 synthcity/plugins/survival_analysis/__init__.py
+-rw-r--r--  2.0 unx     7534 b- defN 23-Apr-27 08:53 synthcity/plugins/survival_analysis/_survival_pipeline.py
+-rw-r--r--  2.0 unx     5873 b- defN 23-Apr-27 08:53 synthcity/plugins/survival_analysis/plugin_survae.py
+-rw-r--r--  2.0 unx     5943 b- defN 23-Apr-27 08:53 synthcity/plugins/survival_analysis/plugin_survival_ctgan.py
+-rw-r--r--  2.0 unx     8082 b- defN 23-Apr-27 08:53 synthcity/plugins/survival_analysis/plugin_survival_gan.py
+-rw-r--r--  2.0 unx     5626 b- defN 23-Apr-27 08:53 synthcity/plugins/survival_analysis/plugin_survival_nflow.py
+-rw-r--r--  2.0 unx      472 b- defN 23-Apr-27 08:53 synthcity/plugins/time_series/__init__.py
+-rw-r--r--  2.0 unx    10563 b- defN 23-Apr-27 08:53 synthcity/plugins/time_series/plugin_fflows.py
+-rw-r--r--  2.0 unx    18792 b- defN 23-Apr-27 08:53 synthcity/plugins/time_series/plugin_timegan.py
+-rw-r--r--  2.0 unx    13933 b- defN 23-Apr-27 08:53 synthcity/plugins/time_series/plugin_timevae.py
+-rw-r--r--  2.0 unx    12146 b- defN 23-Apr-27 08:53 synthcity/utils/anonymization.py
+-rw-r--r--  2.0 unx     2691 b- defN 23-Apr-27 08:53 synthcity/utils/callbacks.py
+-rw-r--r--  2.0 unx     5705 b- defN 23-Apr-27 08:53 synthcity/utils/compression.py
+-rw-r--r--  2.0 unx       98 b- defN 23-Apr-27 08:53 synthcity/utils/constants.py
+-rw-r--r--  2.0 unx      575 b- defN 23-Apr-27 08:53 synthcity/utils/dataframe.py
+-rw-r--r--  2.0 unx     2775 b- defN 23-Apr-27 08:53 synthcity/utils/evaluation.py
+-rw-r--r--  2.0 unx     6482 b- defN 23-Apr-27 08:53 synthcity/utils/optimizer.py
+-rw-r--r--  2.0 unx     1030 b- defN 23-Apr-27 08:53 synthcity/utils/optuna_sample.py
+-rw-r--r--  2.0 unx      653 b- defN 23-Apr-27 08:53 synthcity/utils/redis_wrapper.py
+-rw-r--r--  2.0 unx      473 b- defN 23-Apr-27 08:53 synthcity/utils/reproducibility.py
+-rw-r--r--  2.0 unx    11212 b- defN 23-Apr-27 08:53 synthcity/utils/samplers.py
+-rw-r--r--  2.0 unx     1062 b- defN 23-Apr-27 08:53 synthcity/utils/serialization.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-27 08:53 synthcity/utils/datasets/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-27 08:53 synthcity/utils/datasets/time_series/__init__.py
+-rw-r--r--  2.0 unx     2617 b- defN 23-Apr-27 08:53 synthcity/utils/datasets/time_series/google_stocks.py
+-rw-r--r--  2.0 unx     5973 b- defN 23-Apr-27 08:53 synthcity/utils/datasets/time_series/pbc.py
+-rw-r--r--  2.0 unx     2722 b- defN 23-Apr-27 08:53 synthcity/utils/datasets/time_series/sine.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-27 08:53 synthcity/utils/datasets/time_series/data/__init__.py
+-rw-r--r--  2.0 unx    11357 b- defN 23-Apr-27 08:54 synthcity-0.2.6.dist-info/LICENSE
+-rw-r--r--  2.0 unx    28944 b- defN 23-Apr-27 08:54 synthcity-0.2.6.dist-info/METADATA
+-rw-r--r--  2.0 unx      108 b- defN 23-Apr-27 08:54 synthcity-0.2.6.dist-info/WHEEL
+-rw-r--r--  2.0 unx       10 b- defN 23-Apr-27 08:54 synthcity-0.2.6.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx    15594 b- defN 23-Apr-27 08:54 synthcity-0.2.6.dist-info/RECORD
+154 files, 1339197 bytes uncompressed, 326961 bytes compressed:  75.6%
```

## zipnote {}

```diff
@@ -96,20 +96,26 @@
 
 Filename: synthcity/plugins/core/models/__init__.py
 Comment: 
 
 Filename: synthcity/plugins/core/models/convnet.py
 Comment: 
 
-Filename: synthcity/plugins/core/models/data_encoder.py
+Filename: synthcity/plugins/core/models/factory.py
+Comment: 
+
+Filename: synthcity/plugins/core/models/feature_encoder.py
 Comment: 
 
 Filename: synthcity/plugins/core/models/flows.py
 Comment: 
 
+Filename: synthcity/plugins/core/models/functions.py
+Comment: 
+
 Filename: synthcity/plugins/core/models/gan.py
 Comment: 
 
 Filename: synthcity/plugins/core/models/goggle.py
 Comment: 
 
 Filename: synthcity/plugins/core/models/image_gan.py
@@ -117,14 +123,17 @@
 
 Filename: synthcity/plugins/core/models/layers.py
 Comment: 
 
 Filename: synthcity/plugins/core/models/mlp.py
 Comment: 
 
+Filename: synthcity/plugins/core/models/tabnet.py
+Comment: 
+
 Filename: synthcity/plugins/core/models/tabular_encoder.py
 Comment: 
 
 Filename: synthcity/plugins/core/models/tabular_flows.py
 Comment: 
 
 Filename: synthcity/plugins/core/models/tabular_gan.py
@@ -399,14 +408,17 @@
 
 Filename: synthcity/utils/evaluation.py
 Comment: 
 
 Filename: synthcity/utils/optimizer.py
 Comment: 
 
+Filename: synthcity/utils/optuna_sample.py
+Comment: 
+
 Filename: synthcity/utils/redis_wrapper.py
 Comment: 
 
 Filename: synthcity/utils/reproducibility.py
 Comment: 
 
 Filename: synthcity/utils/samplers.py
@@ -429,23 +441,23 @@
 
 Filename: synthcity/utils/datasets/time_series/sine.py
 Comment: 
 
 Filename: synthcity/utils/datasets/time_series/data/__init__.py
 Comment: 
 
-Filename: synthcity-0.2.5.dist-info/LICENSE
+Filename: synthcity-0.2.6.dist-info/LICENSE
 Comment: 
 
-Filename: synthcity-0.2.5.dist-info/METADATA
+Filename: synthcity-0.2.6.dist-info/METADATA
 Comment: 
 
-Filename: synthcity-0.2.5.dist-info/WHEEL
+Filename: synthcity-0.2.6.dist-info/WHEEL
 Comment: 
 
-Filename: synthcity-0.2.5.dist-info/top_level.txt
+Filename: synthcity-0.2.6.dist-info/top_level.txt
 Comment: 
 
-Filename: synthcity-0.2.5.dist-info/RECORD
+Filename: synthcity-0.2.6.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## synthcity/version.py

```diff
@@ -1,4 +1,4 @@
-__version__ = "0.2.5"
+__version__ = "0.2.6"
 
 MAJOR_VERSION = ".".join(__version__.split(".")[:-1])
 PATCH_VERSION = __version__.split(".")[-1]
```

## synthcity/plugins/core/dataloader.py

```diff
@@ -12,15 +12,15 @@
 from sklearn.model_selection import train_test_split
 from sklearn.preprocessing import LabelEncoder
 from torchvision import transforms
 
 # synthcity absolute
 from synthcity.plugins.core.constraints import Constraints
 from synthcity.plugins.core.dataset import FlexibleDataset, TensorDataset
-from synthcity.plugins.core.models.data_encoder import DatetimeEncoder
+from synthcity.plugins.core.models.feature_encoder import DatetimeEncoder
 from synthcity.utils.compression import compress_dataset, decompress_dataset
 from synthcity.utils.serialization import dataframe_hash
 
 
 class DataLoader(metaclass=ABCMeta):
     """
     .. inheritance-diagram:: synthcity.plugins.core.dataloader.DataLoader
```

## synthcity/plugins/core/distribution.py

```diff
@@ -107,25 +107,33 @@
     @abstractmethod
     def as_constraint(self) -> Constraints:
         """Convert the Distribution to a set of Constraints."""
         ...
 
     @abstractmethod
     def min(self) -> Any:
-        "Get the min value of the distribution"
+        """Get the min value of the distribution."""
         ...
 
     @abstractmethod
     def max(self) -> Any:
-        "Get the max value of the distribution"
+        """Get the max value of the distribution."""
         ...
 
-    @abstractmethod
     def __eq__(self, other: Any) -> bool:
-        ...
+        return type(self) == type(other) and self.get() == other.get()
+
+    def __contains__(self, item: Any) -> bool:
+        """
+        Example:
+        >>> dist = CategoricalDistribution(name="foo", choices=["a", "b", "c"])
+        >>> "a" in dist
+        True
+        """
+        return self.has(item)
 
     @abstractmethod
     def dtype(self) -> str:
         ...
 
 
 class CategoricalDistribution(Distribution):
@@ -142,26 +150,26 @@
         if mkey in values and values[mkey] is not None:
             return list(values[mkey].index)
 
         if len(v) == 0:
             raise ValueError(
                 "Invalid choices for CategoricalDistribution. Provide data or choices params"
             )
-        return v
+        return sorted(set(v))
 
     def get(self) -> List[Any]:
         return [self.name, self.choices]
 
     def sample(self, count: int = 1) -> Any:
         np.random.seed(self.random_state)
         msamples = self.sample_marginal(count)
         if msamples is not None:
             return msamples
 
-        return np.random.choice(self.choices, count).tolist()
+        return np.random.choice(self.choices, count)
 
     def has(self, val: Any) -> bool:
         return val in self.choices
 
     def includes(self, other: "Distribution") -> bool:
         if not isinstance(other, CategoricalDistribution):
             return False
@@ -172,20 +180,14 @@
 
     def min(self) -> Any:
         return min(self.choices)
 
     def max(self) -> Any:
         return max(self.choices)
 
-    def __eq__(self, other: Any) -> bool:
-        if not isinstance(other, CategoricalDistribution):
-            return False
-
-        return self.name == other.name and set(self.choices) == set(other.choices)
-
     def dtype(self) -> str:
         types = {
             "object": 0,
             "float": 0,
             "int": 0,
         }
         for v in self.choices:
@@ -205,16 +207,16 @@
 
 class FloatDistribution(Distribution):
     """
     .. inheritance-diagram:: synthcity.plugins.core.distribution.FloatDistribution
         :parts: 1
     """
 
-    low: float = np.iinfo(np.int64).min
-    high: float = np.iinfo(np.int64).max
+    low: float = np.finfo(np.float64).min
+    high: float = np.finfo(np.float64).max
 
     @validator("low", always=True)
     def _validate_low_thresh(cls: Any, v: float, values: Dict) -> float:
         mkey = "marginal_distribution"
         if mkey in values and values[mkey] is not None:
             return values[mkey].index.min()
 
@@ -255,28 +257,34 @@
 
     def min(self) -> Any:
         return self.low
 
     def max(self) -> Any:
         return self.high
 
-    def __eq__(self, other: Any) -> bool:
-        if not isinstance(other, FloatDistribution):
-            return False
-
-        return (
-            self.name == other.name
-            and self.low == other.low
-            and self.high == other.high
-        )
-
     def dtype(self) -> str:
         return "float"
 
 
+class LogDistribution(FloatDistribution):
+    low: float = np.finfo(np.float64).tiny
+    high: float = np.finfo(np.float64).max
+
+    def get(self) -> List[Any]:
+        return [self.name, self.low, self.high]
+
+    def sample(self, count: int = 1) -> Any:
+        np.random.seed(self.random_state)
+        msamples = self.sample_marginal(count)
+        if msamples is not None:
+            return msamples
+        lo, hi = np.log2(self.low), np.log2(self.high)
+        return 2.0 ** np.random.uniform(lo, hi, count)
+
+
 class IntegerDistribution(Distribution):
     """
     .. inheritance-diagram:: synthcity.plugins.core.distribution.IntegerDistribution
         :parts: 1
     """
 
     low: int = np.iinfo(np.int64).min
@@ -294,25 +302,32 @@
     @validator("high", always=True)
     def _validate_high_thresh(cls: Any, v: int, values: Dict) -> int:
         mkey = "marginal_distribution"
         if mkey in values and values[mkey] is not None:
             return int(values[mkey].index.max())
         return v
 
+    @validator("step", always=True)
+    def _validate_step(cls: Any, v: int, values: Dict) -> int:
+        if v < 1:
+            raise ValueError("Step must be greater than 0")
+        return v
+
     def get(self) -> List[Any]:
         return [self.name, self.low, self.high, self.step]
 
     def sample(self, count: int = 1) -> Any:
         np.random.seed(self.random_state)
         msamples = self.sample_marginal(count)
         if msamples is not None:
             return msamples
 
-        choices = [val for val in range(self.low, self.high + 1, self.step)]
-        return np.random.choice(choices, count).tolist()
+        steps = (self.high - self.low) // self.step
+        samples = np.random.choice(steps + 1, count)
+        return samples * self.step + self.low
 
     def has(self, val: Any) -> bool:
         return self.low <= val and val <= self.high
 
     def includes(self, other: "Distribution") -> bool:
         return self.min() <= other.min() and other.max() <= self.max()
 
@@ -327,82 +342,87 @@
 
     def min(self) -> Any:
         return self.low
 
     def max(self) -> Any:
         return self.high
 
-    def __eq__(self, other: Any) -> bool:
-        if not isinstance(other, IntegerDistribution):
-            return False
-
-        return (
-            self.name == other.name
-            and self.low == other.low
-            and self.high == other.high
-        )
-
     def dtype(self) -> str:
         return "int"
 
 
-OFFSET = 120
+class IntLogDistribution(IntegerDistribution):
+    low: int = 1
+    high: int = np.iinfo(np.int64).max
+
+    @validator("step", always=True)
+    def _validate_step(cls: Any, v: int, values: Dict) -> int:
+        if v != 1:
+            raise ValueError("Step must be 1 for IntLogDistribution")
+        return v
+
+    def get(self) -> List[Any]:
+        return [self.name, self.low, self.high]
+
+    def sample(self, count: int = 1) -> Any:
+        np.random.seed(self.random_state)
+        msamples = self.sample_marginal(count)
+        if msamples is not None:
+            return msamples
+        lo, hi = np.log2(self.low), np.log2(self.high)
+        samples = 2.0 ** np.random.uniform(lo, hi, count)
+        return samples.astype(int)
 
 
 class DatetimeDistribution(Distribution):
     """
     .. inheritance-diagram:: synthcity.plugins.core.distribution.DatetimeDistribution
         :parts: 1
     """
 
     low: datetime = datetime.utcfromtimestamp(0)
     high: datetime = datetime.now()
+    step: timedelta = timedelta(microseconds=1)
+    offset: timedelta = timedelta(seconds=120)
 
     @validator("low", always=True)
     def _validate_low_thresh(cls: Any, v: datetime, values: Dict) -> datetime:
         mkey = "marginal_distribution"
         if mkey in values and values[mkey] is not None:
             v = values[mkey].index.min()
-
-        return v - timedelta(seconds=OFFSET)
+        return v
 
     @validator("high", always=True)
     def _validate_high_thresh(cls: Any, v: datetime, values: Dict) -> datetime:
         mkey = "marginal_distribution"
         if mkey in values and values[mkey] is not None:
             v = values[mkey].index.max()
-
-        return v + timedelta(seconds=OFFSET)
+        return v
 
     def get(self) -> List[Any]:
-        return [self.name, self.low, self.high]
+        return [self.name, self.low, self.high, self.step, self.offset]
 
     def sample(self, count: int = 1) -> Any:
         np.random.seed(self.random_state)
         msamples = self.sample_marginal(count)
         if msamples is not None:
             return msamples
 
-        samples = np.random.uniform(
-            datetime.timestamp(self.low), datetime.timestamp(self.high), count
-        )
-
-        samples_dt = []
-        for s in samples:
-            samples_dt.append(datetime.fromtimestamp(s))
-
-        return samples_dt
+        n = (self.high - self.low) // self.step + 1
+        samples = np.round(np.random.rand(count) * n - 0.5)
+        return self.low + samples * self.step
 
     def has(self, val: datetime) -> bool:
         return self.low <= val and val <= self.high
 
     def includes(self, other: "Distribution") -> bool:
-        return self.min() - timedelta(
-            seconds=OFFSET
-        ) <= other.min() and other.max() <= self.max() + timedelta(seconds=OFFSET)
+        return (
+            self.min() - self.offset <= other.min()
+            and other.max() <= self.max() + self.offset
+        )
 
     def as_constraint(self) -> Constraints:
         return Constraints(
             rules=[
                 (self.name, "le", self.high),
                 (self.name, "ge", self.low),
                 (self.name, "dtype", "datetime"),
@@ -411,24 +431,14 @@
 
     def min(self) -> Any:
         return self.low
 
     def max(self) -> Any:
         return self.high
 
-    def __eq__(self, other: Any) -> bool:
-        if not isinstance(other, DatetimeDistribution):
-            return False
-
-        return (
-            self.name == other.name
-            and self.low == other.low
-            and self.high == other.high
-        )
-
     def dtype(self) -> str:
         return "datetime"
 
 
 def constraint_to_distribution(constraints: Constraints, feature: str) -> Distribution:
     """Infer Distribution from Constraints.
```

## synthcity/plugins/core/schema.py

```diff
@@ -69,15 +69,15 @@
 
         if sampling_strategy == "marginal":
             for col in X.columns:
                 if X[col].dtype.kind in ["O", "b"] or len(X[col].unique()) < 10:
                     feature_domain[col] = CategoricalDistribution(
                         name=col, data=X[col], random_state=random_state
                     )
-                elif X[col].dtype.kind == "i":
+                elif X[col].dtype.kind in ["i", "u"]:
                     feature_domain[col] = IntegerDistribution(
                         name=col, data=X[col], random_state=random_state
                     )
                 elif X[col].dtype.kind == "f":
                     feature_domain[col] = FloatDistribution(
                         name=col, data=X[col], random_state=random_state
                     )
@@ -91,15 +91,15 @@
             for col in X.columns:
                 if X[col].dtype.kind in ["O", "b"] or len(X[col].unique()) < 10:
                     feature_domain[col] = CategoricalDistribution(
                         name=col,
                         choices=list(X[col].unique()),
                         random_state=random_state,
                     )
-                elif X[col].dtype.kind == "i":
+                elif X[col].dtype.kind in ["i", "u"]:
                     feature_domain[col] = IntegerDistribution(
                         name=col,
                         low=X[col].min(),
                         high=X[col].max(),
                         random_state=random_state,
                     )
                 elif X[col].dtype.kind == "f":
```

## synthcity/plugins/core/models/convnet.py

```diff
@@ -65,16 +65,16 @@
     early_stopping: bool
         Enable/disable early stopping
     """
 
     @validate_arguments(config=dict(arbitrary_types_allowed=True))
     def __init__(
         self,
-        task_type: str,
-        model: nn.Module,  # classification/regression
+        task_type: str,  # classification/regression
+        model: nn.Module,
         lr: float = 1e-3,
         weight_decay: float = 1e-3,
         opt_betas: tuple = (0.9, 0.999),
         n_iter: int = 1000,
         batch_size: int = 500,
         n_iter_print: int = 100,
         random_state: int = 0,
```

## synthcity/plugins/core/models/goggle.py

```diff
@@ -15,19 +15,22 @@
 from torch_geometric.nn.conv import MessagePassing
 from torch_geometric.utils import dense_to_sparse
 from tqdm import tqdm
 
 # synthcity absolute
 import synthcity.logger as log
 from synthcity.plugins.core.dataloader import DataLoader
-from synthcity.plugins.core.models.mlp import MultiActivationHead, get_nonlin
-from synthcity.plugins.core.models.RGCNConv import RGCNConv
 from synthcity.utils.constants import DEVICE
 from synthcity.utils.reproducibility import clear_cache, enable_reproducible_results
 
+# synthcity relative
+from .factory import get_nonlin
+from .layers import MultiActivationHead
+from .RGCNConv import RGCNConv
+
 
 class Goggle(nn.Module):
     @validate_arguments(config=dict(arbitrary_types_allowed=True))
     def __init__(
         self,
         input_dim: int,
         n_iter: int = 1000,
```

## synthcity/plugins/core/models/layers.py

```diff
@@ -1,14 +1,22 @@
 # stdlib
-from typing import Any, Optional
+from typing import Any, List, Optional, Tuple, Type
 
 # third party
+import numpy as np
 import torch
+from pydantic import validate_arguments
 from torch import nn
 
+# synthcity absolute
+from synthcity.utils.constants import DEVICE
+
+# synthcity relative
+from .functions import EntmaxFunction, SparsemaxFunction
+
 
 class Permute(nn.Module):
     def __init__(self, *dims: Any) -> None:
         super(Permute, self).__init__()
         self.dims = dims
 
     def forward(self, x: torch.Tensor) -> torch.Tensor:
@@ -30,7 +38,128 @@
         self.dims, self.contiguous = dims, contiguous
 
     def forward(self, x: torch.Tensor) -> torch.Tensor:
         if self.contiguous:
             return x.transpose(*self.dims).contiguous()
         else:
             return x.transpose(*self.dims)
+
+
+@validate_arguments(config=dict(arbitrary_types_allowed=True))
+def _forward_skip_connection(
+    self: nn.Module, X: torch.Tensor, *args: Any, **kwargs: Any
+) -> torch.Tensor:
+    # if X.shape[-1] == 0:
+    #     return torch.zeros((*X.shape[:-1], self.n_units_out)).to(self.device)
+    X = X.float().to(self.device)
+    out = self._forward(X, *args, **kwargs)
+    return torch.cat([out, X], dim=-1)
+
+
+def SkipConnection(cls: Type[nn.Module]) -> Type[nn.Module]:
+    """Wraps a model to add a skip connection from the input to the output.
+
+    Example:
+    >>> ResidualBlock = SkipConnection(MLP)
+    >>> res_block = ResidualBlock(n_units_in=10, n_units_out=3, n_units_hidden=64)
+    >>> res_block(torch.ones(10, 10)).shape
+    (10, 13)
+    """
+
+    class Wrapper(cls):  # type: ignore
+        device: torch.device = DEVICE
+
+    Wrapper._forward = cls.forward
+    Wrapper.forward = _forward_skip_connection
+    Wrapper.__name__ = f"SkipConnection({cls.__name__})"
+    Wrapper.__qualname__ = f"SkipConnection({cls.__qualname__})"
+    Wrapper.__doc__ = f"""(With skipped connection) {cls.__doc__}"""
+    return Wrapper
+
+
+# class GLU(nn.Module):
+#     """Gated Linear Unit (GLU)."""
+
+#     def __init__(self, activation: Union[str, nn.Module] = "sigmoid") -> None:
+#         super().__init__()
+#         if type(activation) == str:
+#             self.non_lin = get_nonlin(activation)
+#         else:
+#             self.non_lin = activation
+
+#     def forward(self, x: Tensor) -> Tensor:
+#         if x.shape[-1] % 2:
+#             raise ValueError("The last dimension of the input tensor must be even.")
+#         a, b = x.chunk(2, dim=-1)
+#         return a * self.non_lin(b)
+
+
+class GumbelSoftmax(nn.Module):
+    def __init__(
+        self, tau: float = 0.2, hard: bool = False, eps: float = 1e-10, dim: int = -1
+    ) -> None:
+        super(GumbelSoftmax, self).__init__()
+
+        self.tau = tau
+        self.hard = hard
+        self.eps = eps
+        self.dim = dim
+
+    def forward(self, logits: torch.Tensor) -> torch.Tensor:
+        return nn.functional.gumbel_softmax(
+            logits, tau=self.tau, hard=self.hard, eps=self.eps, dim=self.dim
+        )
+
+
+class MultiActivationHead(nn.Module):
+    """Final layer with multiple activations. Useful for tabular data."""
+
+    def __init__(
+        self,
+        activations: List[Tuple[nn.Module, int]],
+        device: Any = DEVICE,
+    ) -> None:
+        super(MultiActivationHead, self).__init__()
+        self.activations = []
+        self.activation_lengths = []
+        self.device = device
+
+        for activation, length in activations:
+            self.activations.append(activation)
+            self.activation_lengths.append(length)
+
+    @validate_arguments(config=dict(arbitrary_types_allowed=True))
+    def forward(self, X: torch.Tensor) -> torch.Tensor:
+        if X.shape[-1] != np.sum(self.activation_lengths):
+            raise RuntimeError(
+                f"Shape mismatch for the activations: expected {np.sum(self.activation_lengths)}. Got shape {X.shape}."
+            )
+
+        split = 0
+        out = torch.zeros(X.shape).to(self.device)
+
+        for activation, step in zip(self.activations, self.activation_lengths):
+            out[..., split : split + step] = activation(X[..., split : split + step])
+
+            split += step
+
+        return out
+
+
+class Sparsemax(nn.Module):
+    def __init__(self, dim: int = -1) -> None:
+        super(Sparsemax, self).__init__()
+        self.dim = dim
+
+    @validate_arguments(config=dict(arbitrary_types_allowed=True))
+    def forward(self, input: torch.Tensor) -> torch.Tensor:
+        return SparsemaxFunction.apply(input, self.dim)
+
+
+class Entmax(nn.Module):
+    def __init__(self, dim: int = -1) -> None:
+        super(Entmax, self).__init__()
+        self.dim = dim
+
+    @validate_arguments(config=dict(arbitrary_types_allowed=True))
+    def forward(self, input: torch.Tensor) -> torch.Tensor:
+        return EntmaxFunction.apply(input, self.dim)
```

## synthcity/plugins/core/models/mlp.py

```diff
@@ -1,90 +1,29 @@
 # stdlib
-from typing import Any, Callable, List, Optional, Tuple, Union
+from typing import Any, Callable, List, Optional, Tuple
 
 # third party
 import numpy as np
 import torch
 from pydantic import validate_arguments
-from torch import Tensor, nn
+from torch import nn
 from torch.utils.data import DataLoader, TensorDataset
 
 # synthcity absolute
 import synthcity.logger as log
+from synthcity.plugins.core.models.factory import get_nonlin
+from synthcity.plugins.core.models.layers import (
+    GumbelSoftmax,
+    MultiActivationHead,
+    SkipConnection,
+)
 from synthcity.utils.constants import DEVICE
 from synthcity.utils.reproducibility import enable_reproducible_results
 
 
-class GumbelSoftmax(nn.Module):
-    def __init__(
-        self, tau: float = 0.2, hard: bool = False, eps: float = 1e-10, dim: int = -1
-    ) -> None:
-        super(GumbelSoftmax, self).__init__()
-
-        self.tau = tau
-        self.hard = hard
-        self.eps = eps
-        self.dim = dim
-
-    def forward(self, logits: torch.Tensor) -> torch.Tensor:
-        return nn.functional.gumbel_softmax(
-            logits, tau=self.tau, hard=self.hard, eps=self.eps, dim=self.dim
-        )
-
-
-class GLU(nn.Module):
-    """Gated Linear Unit (GLU)."""
-
-    def __init__(self, activation: Union[str, nn.Module] = "sigmoid") -> None:
-        super().__init__()
-        if type(activation) == str:
-            self.non_lin = get_nonlin(activation)
-        else:
-            self.non_lin = activation
-
-    def forward(self, x: Tensor) -> Tensor:
-        if x.shape[-1] % 2:
-            raise ValueError("The last dimension of the input tensor must be even.")
-        a, b = x.chunk(2, dim=-1)
-        return a * self.non_lin(b)
-
-
-def get_nonlin(name: Union[str, nn.Module]) -> nn.Module:
-    if isinstance(name, nn.Module):
-        return name
-    elif name == "none":
-        return nn.Identity()
-    elif name == "elu":
-        return nn.ELU()
-    elif name == "relu":
-        return nn.ReLU()
-    elif name == "leaky_relu":
-        return nn.LeakyReLU()
-    elif name == "selu":
-        return nn.SELU()
-    elif name == "tanh":
-        return nn.Tanh()
-    elif name == "sigmoid":
-        return nn.Sigmoid()
-    elif name == "softmax":
-        return GumbelSoftmax()
-    elif name == "gelu":
-        return nn.GELU()
-    elif name == "glu":
-        return GLU()
-    elif name == "reglu":
-        return GLU("relu")
-    elif name == "geglu":
-        return GLU("gelu")
-    elif name in ("silu", "swish"):
-        return nn.SiLU()
-    else:
-        raise ValueError(f"Unknown nonlinearity {name}")
-
-
 class LinearLayer(nn.Module):
     @validate_arguments(config=dict(arbitrary_types_allowed=True))
     def __init__(
         self,
         n_units_in: int,
         n_units_out: int,
         dropout: float = 0,
@@ -110,78 +49,15 @@
         self.model = nn.Sequential(*layers).to(self.device)
 
     @validate_arguments(config=dict(arbitrary_types_allowed=True))
     def forward(self, X: torch.Tensor) -> torch.Tensor:
         return self.model(X.float()).to(self.device)
 
 
-class ResidualLayer(LinearLayer):
-    @validate_arguments(config=dict(arbitrary_types_allowed=True))
-    def __init__(
-        self,
-        n_units_in: int,
-        n_units_out: int,
-        dropout: float = 0,
-        batch_norm: bool = False,
-        nonlin: Optional[str] = "relu",
-        device: Any = DEVICE,
-    ) -> None:
-        super(ResidualLayer, self).__init__(
-            n_units_in,
-            n_units_out,
-            dropout=dropout,
-            batch_norm=batch_norm,
-            nonlin=nonlin,
-            device=device,
-        )
-        self.device = device
-        self.n_units_out = n_units_out
-
-    @validate_arguments(config=dict(arbitrary_types_allowed=True))
-    def forward(self, X: torch.Tensor) -> torch.Tensor:
-        if X.shape[-1] == 0:
-            return torch.zeros((*X.shape[:-1], self.n_units_out)).to(self.device)
-
-        out = self.model(X.float())
-        return torch.cat([out, X], dim=-1).to(self.device)
-
-
-class MultiActivationHead(nn.Module):
-    """Final layer with multiple activations. Useful for tabular data."""
-
-    def __init__(
-        self,
-        activations: List[Tuple[nn.Module, int]],
-        device: Any = DEVICE,
-    ) -> None:
-        super(MultiActivationHead, self).__init__()
-        self.activations = []
-        self.activation_lengths = []
-        self.device = device
-
-        for activation, length in activations:
-            self.activations.append(activation)
-            self.activation_lengths.append(length)
-
-    @validate_arguments(config=dict(arbitrary_types_allowed=True))
-    def forward(self, X: torch.Tensor) -> torch.Tensor:
-        if X.shape[-1] != np.sum(self.activation_lengths):
-            raise RuntimeError(
-                f"Shape mismatch for the activations: expected {np.sum(self.activation_lengths)}. Got shape {X.shape}."
-            )
-
-        split = 0
-        out = torch.zeros(X.shape).to(self.device)
-
-        for activation, step in zip(self.activations, self.activation_lengths):
-            out[..., split : split + step] = activation(X[..., split : split + step])
-
-            split += step
-
-        return out
+ResidualLayer = SkipConnection(LinearLayer)
 
 
 class MLP(nn.Module):
     """
     .. inheritance-diagram:: synthcity.plugins.core.models.mlp.MLP
         :parts: 1
 
@@ -231,17 +107,18 @@
     loss: Callable
         Optional Custom loss function. If None, the loss is CrossEntropy for classification tasks, or RMSE for regression.
     """
 
     @validate_arguments(config=dict(arbitrary_types_allowed=True))
     def __init__(
         self,
-        task_type: str,  # classification/regression
         n_units_in: int,
         n_units_out: int,
+        *,
+        task_type: str = "regression",  # classification/regression
         n_layers_hidden: int = 1,
         n_units_hidden: int = 100,
         nonlin: str = "relu",
         nonlin_out: Optional[List[Tuple[str, int]]] = None,
         lr: float = 1e-3,
         weight_decay: float = 1e-3,
         opt_betas: tuple = (0.9, 0.999),
```

## synthcity/plugins/core/models/tabular_encoder.py

```diff
@@ -1,35 +1,36 @@
 """TabularEncoder module.
 """
 
 # stdlib
-from typing import Any, List, Optional, Sequence, Tuple
+from typing import Any, List, Optional, Sequence, Tuple, Union
 
 # third party
 import numpy as np
 import pandas as pd
 from pydantic import BaseModel, validate_arguments, validator
 from sklearn.base import BaseEstimator, TransformerMixin
-from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
+from sklearn.preprocessing import MinMaxScaler
 
 # synthcity absolute
 import synthcity.logger as log
 from synthcity.utils.dataframe import discrete_columns as find_cat_cols
 from synthcity.utils.serialization import dataframe_hash
 
 # synthcity relative
-from .data_encoder import ContinuousDataEncoder
+from .factory import get_feature_encoder
 
 
 class FeatureInfo(BaseModel):
     name: str
     feature_type: str
     transform: Any
     output_dimensions: int
     transformed_features: List[str]
+    trans_feature_types: List[str]
 
     @validator("feature_type")
     def _feature_type_validator(cls: Any, v: str) -> str:
         if v not in ["discrete", "continuous"]:
             raise ValueError(f"Invalid feature type {v}")
         return v
 
@@ -46,407 +47,287 @@
     @validator("output_dimensions")
     def _output_dimensions_validator(cls: Any, v: int) -> int:
         if v <= 0:
             raise ValueError(f"Invalid output_dimensions {v}")
         return v
 
 
-class BinEncoder(TransformerMixin, BaseEstimator):
-    """Binary encoder (for SurvivalGAN).
-
-    Model continuous columns with a BayesianGMM and normalized to a scalar [0, 1] and a vector.
-    Discrete columns are encoded using a scikit-learn OneHotEncoder.
-    """
-
-    @validate_arguments(config=dict(arbitrary_types_allowed=True))
-    def __init__(
-        self,
-        max_clusters: int = 10,
-        categorical_limit: int = 10,
-    ) -> None:
-        """Create a data transformer.
-
-        Args:
-            max_clusters (int):
-                Maximum number of Gaussian distributions in Bayesian GMM.
-        """
-        self.max_clusters = max_clusters
-        self.categorical_limit = categorical_limit
-
-    @validate_arguments(config=dict(arbitrary_types_allowed=True))
-    def _fit_continuous(self, data: pd.Series) -> FeatureInfo:
-        """Train Bayesian GMM for continuous columns.
-
-        Args:
-            data (pd.Series):
-                A dataframe containing a column.
-
-        Returns:
-            namedtuple:
-                A ``FeatureInfo`` object.
-        """
-        name = data.name
-        encoder = ContinuousDataEncoder(
-            n_components=min(self.max_clusters, len(data)),
-        )
-        encoder.fit(data)
-        num_components = encoder.components()
-
-        transformed_features = [f"{name}.value"] + [
-            f"{name}.component_{i}" for i in range(num_components)
-        ]
-
-        return FeatureInfo(
-            name=name,
-            feature_type="continuous",
-            transform=encoder,
-            output_dimensions=1 + num_components,
-            transformed_features=transformed_features,
-        )
-
-    def fit(
-        self, raw_data: pd.DataFrame, discrete_columns: Optional[List] = None
-    ) -> "BinEncoder":
-        """Fit the ``BinEncoder``.
-
-        Fits a ``ContinuousDataEncoder`` for continuous columns
-        """
-        if discrete_columns is None:
-            discrete_columns = find_cat_cols(raw_data, self.categorical_limit)
-
-        self.output_dimensions = 0
-
-        self._column_transform_info = {}
-        for name in raw_data.columns:
-            if name not in discrete_columns:
-                column_transform_info = self._fit_continuous(raw_data[name])
-                self._column_transform_info[name] = column_transform_info
-
-        return self
-
-    def _transform_continuous(
-        self, column_transform_info: FeatureInfo, data: pd.Series
-    ) -> pd.Series:
-        name = data.name
-        encoder = column_transform_info.transform
-        transformed = encoder.transform(data)
-
-        return transformed[f"{name}.component"].to_numpy().astype(int)
-
-    @validate_arguments(config=dict(arbitrary_types_allowed=True))
-    def transform(self, raw_data: pd.DataFrame) -> pd.DataFrame:
-        """Take raw data and output a matrix data."""
-        output = raw_data.copy()
-
-        for name in self._column_transform_info:
-            column_transform_info = self._column_transform_info[name]
-
-            output[name] = self._transform_continuous(
-                column_transform_info, raw_data[name]
-            )
-
-        return output
-
-    @validate_arguments(config=dict(arbitrary_types_allowed=True))
-    def fit_transform(self, raw_data: pd.DataFrame) -> pd.DataFrame:
-        return self.fit(raw_data).transform(raw_data)
-
-
 class TabularEncoder(TransformerMixin, BaseEstimator):
     """Tabular encoder.
 
     Model continuous columns with a BayesianGMM and normalized to a scalar [0, 1] and a vector.
     Discrete columns are encoded using a scikit-learn OneHotEncoder.
     """
 
+    categorical_encoder: Union[str, type] = "onehot"
+    continuous_encoder: Union[str, type] = "bayesian_gmm"
+    cat_encoder_params: dict = dict(handle_unknown="ignore", sparse=False)
+    cont_encoder_params: dict = dict(n_components=10)
+
     @validate_arguments(config=dict(arbitrary_types_allowed=True))
     def __init__(
         self,
+        *,
+        whitelist: tuple = (),
         max_clusters: int = 10,
         categorical_limit: int = 10,
-        whitelist: list = [],
+        categorical_encoder: Optional[Union[str, type]] = None,
+        continuous_encoder: Optional[Union[str, type]] = None,
+        cat_encoder_params: Optional[dict] = None,
+        cont_encoder_params: Optional[dict] = None,
     ) -> None:
         """Create a data transformer.
 
         Args:
-            max_clusters (int):
-                Maximum number of Gaussian distributions in Bayesian GMM.
+            whitelist (tuple):
+                Columns that will not be transformed.
         """
-        self.max_clusters = max_clusters
-        self.categorical_limit = categorical_limit
         self.whitelist = whitelist
+        self.categorical_limit = categorical_limit
+        self.max_clusters = max_clusters
+        if categorical_encoder is not None:
+            self.categorical_encoder = categorical_encoder
+        if continuous_encoder is not None:
+            self.continuous_encoder = continuous_encoder
+        if cat_encoder_params is not None:
+            self.cat_encoder_params = cat_encoder_params
+        else:
+            self.cat_encoder_params = self.cat_encoder_params.copy()
+        if cont_encoder_params is not None:
+            self.cont_encoder_params = cont_encoder_params
+        else:
+            self.cont_encoder_params = self.cont_encoder_params.copy()
+        if self.continuous_encoder == "bayesian_gmm":
+            self.cont_encoder_params["n_components"] = max_clusters
 
     @validate_arguments(config=dict(arbitrary_types_allowed=True))
-    def _fit_continuous(self, data: pd.Series) -> FeatureInfo:
-        """Train Bayesian GMM for continuous columns.
-
-        Args:
-            data (pd.DataFrame):
-                A dataframe containing a column.
-
-        Returns:
-            namedtuple:
-                A ``FeatureInfo`` object.
-        """
-        name = data.name
-        encoder = ContinuousDataEncoder(
-            n_components=min(len(data), self.max_clusters),
-        )
-        encoder.fit(data)
-        num_components = encoder.components()
-
-        transformed_features = [f"{name}.value"] + [
-            f"{name}.component_{i}" for i in range(num_components)
-        ]
-        return FeatureInfo(
-            name=name,
-            feature_type="continuous",
-            transform=encoder,
-            output_dimensions=1 + num_components,
-            transformed_features=transformed_features,
-        )
-
-    @validate_arguments(config=dict(arbitrary_types_allowed=True))
-    def _fit_discrete(self, data: pd.Series) -> FeatureInfo:
-        """Fit one hot encoder for discrete column.
+    def _fit_feature(self, feature: pd.Series, feature_type: str) -> FeatureInfo:
+        """Fit the feature encoder on a column.
 
         Args:
-            data (pd.DataFrame):
-                A dataframe containing a column.
+            feature (pd.Series):
+                A column of a dataframe.
+            feature_type (str):
+                Type of the feature ('discrete' or 'continuous').
 
         Returns:
-            namedtuple:
-                A ``FeatureInfo`` object.
+            FeatureInfo:
+                Information of the fitted feature encoder.
         """
-        name = data.name
-        ohe = OneHotEncoder(handle_unknown="ignore", sparse=False)
-        ohe.fit(data.values.reshape(-1, 1))
-        num_categories = len(ohe.categories_[0])
+        if feature_type == "discrete":
+            encoder = get_feature_encoder(
+                self.categorical_encoder, self.cat_encoder_params
+            )
+        else:
+            encoder = get_feature_encoder(
+                self.continuous_encoder, self.cont_encoder_params
+            )
 
-        transformed_features = list(ohe.get_feature_names_out([data.name]))
+        encoder.fit(feature)
 
         return FeatureInfo(
-            name=name,
-            feature_type="discrete",
-            transform=ohe,
-            output_dimensions=num_categories,
-            transformed_features=transformed_features,
+            name=feature.name,
+            feature_type=feature_type,
+            transform=encoder,
+            output_dimensions=encoder.n_features_out,
+            transformed_features=encoder.feature_names_out,
+            trans_feature_types=encoder.feature_types_out,
         )
 
     @validate_arguments(config=dict(arbitrary_types_allowed=True))
     def fit(
         self, raw_data: pd.DataFrame, discrete_columns: Optional[List] = None
     ) -> Any:
         """Fit the ``TabularEncoder``.
 
-        Fits a ``ContinuousDataEncoder`` for continuous columns and a
-        ``OneHotEncoder`` for discrete columns.
-
         This step also counts the #columns in matrix data and span information.
         """
         if discrete_columns is None:
             discrete_columns = find_cat_cols(raw_data, self.categorical_limit)
+
         self.output_dimensions = 0
 
         self._column_raw_dtypes = raw_data.infer_objects().dtypes
-        self._column_transform_info_list = []
+        self._column_transform_info_list: Sequence[FeatureInfo] = []
 
         for name in raw_data.columns:
             if name in self.whitelist:
                 continue
             column_hash = dataframe_hash(raw_data[[name]])
             log.info(f"Encoding {name} {column_hash}")
-
-            if name in discrete_columns:
-                column_transform_info = self._fit_discrete(raw_data[name])
-            else:
-                column_transform_info = self._fit_continuous(raw_data[name])
+            ftype = "discrete" if name in discrete_columns else "continuous"
+            column_transform_info = self._fit_feature(raw_data[name], ftype)
 
             self.output_dimensions += column_transform_info.output_dimensions
             self._column_transform_info_list.append(column_transform_info)
+
         return self
 
-    def _transform_continuous(
-        self, column_transform_info: FeatureInfo, data: pd.Series
+    def _transform_feature(
+        self, column_transform_info: FeatureInfo, feature: pd.Series
     ) -> pd.DataFrame:
-        name = data.name
         encoder = column_transform_info.transform
-        transformed = encoder.transform(data)
-
-        #  Converts the transformed data to the appropriate output format.
-        output = np.zeros((len(transformed), column_transform_info.output_dimensions))
-        output[:, 0] = transformed[f"{name}.value"].to_numpy()
-        index = transformed[f"{name}.component"].to_numpy().astype(int)
-        output[np.arange(index.size), index + 1] = 1
-
-        return pd.DataFrame(
-            output,
-            columns=column_transform_info.transformed_features,
-        )
-
-    def _transform_discrete(
-        self, column_transform_info: FeatureInfo, data: pd.Series
-    ) -> pd.DataFrame:
-        ohe = column_transform_info.transform
         return pd.DataFrame(
-            ohe.transform(data.to_frame().values),
+            encoder.transform(feature).values,
             columns=column_transform_info.transformed_features,
         )
 
     @validate_arguments(config=dict(arbitrary_types_allowed=True))
     def transform(self, raw_data: pd.DataFrame) -> pd.DataFrame:
         """Take raw data and output a matrix data."""
         if len(self._column_transform_info_list) == 0:
             return pd.DataFrame(np.zeros((len(raw_data), 0)))
 
         column_data_list = []
         for name in self.whitelist:
             if name not in raw_data.columns:
                 continue
-            data = raw_data[name]
-            column_data_list.append(data)
+            feature = raw_data[name]
+            column_data_list.append(feature)
 
         for column_transform_info in self._column_transform_info_list:
-            name = column_transform_info.name
-            data = raw_data[name]
-
-            if column_transform_info.feature_type == "continuous":
-                column_data_list.append(
-                    self._transform_continuous(column_transform_info, data)
-                )
-            else:
-                column_data_list.append(
-                    self._transform_discrete(column_transform_info, data)
-                )
+            feature = raw_data[column_transform_info.name]
+            column_data_list.append(
+                self._transform_feature(column_transform_info, feature)
+            )
 
         result = pd.concat(column_data_list, axis=1)
         result.index = raw_data.index
 
         return result
 
     @validate_arguments(config=dict(arbitrary_types_allowed=True))
-    def _inverse_transform_continuous(
+    def _inverse_transform_feature(
         self,
         column_transform_info: FeatureInfo,
         column_data: pd.DataFrame,
-    ) -> pd.DataFrame:
+    ) -> pd.Series:
         encoder = column_transform_info.transform
-        data = pd.DataFrame(column_data.values[:, :2], columns=["value", "component"])
-        data.iloc[:, 1] = np.argmax(column_data.values[:, 1:], axis=1)
-        return encoder.inverse_transform(data)
-
-    @validate_arguments(config=dict(arbitrary_types_allowed=True))
-    def _inverse_transform_discrete(
-        self, column_transform_info: FeatureInfo, column_data: pd.DataFrame
-    ) -> pd.DataFrame:
-        ohe = column_transform_info.transform
-        column = column_transform_info.name
-        return pd.DataFrame(
-            ohe.inverse_transform(column_data),
-            columns=[column],
-        )
+        return encoder.inverse_transform(column_data)
 
     @validate_arguments(config=dict(arbitrary_types_allowed=True))
     def inverse_transform(self, data: pd.DataFrame) -> pd.DataFrame:
         """Take matrix data and output raw data.
 
         Output uses the same type as input to the transform function.
         """
         if len(self._column_transform_info_list) == 0:
             return pd.DataFrame(np.zeros((len(data), 0)))
 
         st = 0
-        recovered_column_data_list = []
         names = []
         feature_types = []
+        recovered_feature_list = []
 
         for name in self.whitelist:
             if name not in data.columns:
                 continue
-            local_data = data[name]
             names.append(name)
             feature_types.append(self._column_raw_dtypes)
-            recovered_column_data_list.append(local_data)
+            recovered_feature_list.append(data[name])
 
         for column_transform_info in self._column_transform_info_list:
             dim = column_transform_info.output_dimensions
             column_data = data.iloc[:, list(range(st, st + dim))]
-            if column_transform_info.feature_type == "continuous":
-                recovered_column_data = self._inverse_transform_continuous(
-                    column_transform_info, column_data
-                )
-            else:
-                recovered_column_data = self._inverse_transform_discrete(
-                    column_transform_info, column_data
-                )
-
-            recovered_column_data_list.append(recovered_column_data)
+            recovered_feature = self._inverse_transform_feature(
+                column_transform_info, column_data
+            )
+            recovered_feature_list.append(recovered_feature)
             names.append(column_transform_info.name)
             st += dim
 
-        recovered_data = np.column_stack(recovered_column_data_list)
+        recovered_data = np.column_stack(recovered_feature_list)
         recovered_data = pd.DataFrame(
             recovered_data, columns=names, index=data.index
         ).astype(self._column_raw_dtypes.filter(names))
         return recovered_data
 
-    def layout(self) -> List[Tuple]:
+    def layout(self) -> Sequence[FeatureInfo]:
         """Get the layout of the encoded dataset.
 
         Returns a list of tuple, describing each column as:
             - continuous, and with length 1 + number of GMM clusters.
             - discrete, and with length <N>, the length of the one-hot encoding.
         """
         return self._column_transform_info_list
 
     def n_features(self) -> int:
         return np.sum(
-            [
-                column_transform_info.output_dimensions
-                for column_transform_info in self._column_transform_info_list
-            ]
+            column_transform_info.output_dimensions
+            for column_transform_info in self._column_transform_info_list
         )
 
     def get_column_info(self, name: str) -> FeatureInfo:
         for column_transform_info in self._column_transform_info_list:
             if column_transform_info.name == name:
                 return column_transform_info
 
         raise RuntimeError(f"Unknown column {name}")
 
     @validate_arguments(config=dict(arbitrary_types_allowed=True))
     def activation_layout(
         self, discrete_activation: str, continuous_activation: str
-    ) -> Sequence[Tuple]:
+    ) -> Sequence[Tuple[str, int]]:
         """Get the layout of the activations.
 
         Returns a list of tuple, describing each column as:
             - continuous, and with length 1 + number of GMM clusters.
             - discrete, and with length <N>, the length of the one-hot encoding.
         """
         out = []
+        acts = dict(discrete=discrete_activation, continuous=continuous_activation)
         for column_transform_info in self._column_transform_info_list:
-            if column_transform_info.feature_type == "continuous":
-                out.extend(
-                    [
-                        (continuous_activation, 1),
-                        (
-                            discrete_activation,
-                            column_transform_info.output_dimensions - 1,
-                        ),
-                    ]
-                )
-            else:
-                out.append(
-                    (discrete_activation, column_transform_info.output_dimensions)
-                )
-
+            ct = column_transform_info.trans_feature_types[0]
+            d = 0
+            for t in column_transform_info.trans_feature_types:
+                if t != ct:
+                    out.append((acts[ct], d))
+                    ct = t
+                    d = 0
+                d += 1
+            out.append((acts[ct], d))
         return out
 
 
+class BinEncoder(TabularEncoder):
+    """Binary encoder (for SurvivalGAN).
+
+    Model continuous columns with a BayesianGMM and normalized to a scalar [0, 1] and a vector.
+    Discrete columns are encoded using a scikit-learn OneHotEncoder.
+    """
+
+    continuous_encoder = "bayesian_gmm"
+    cont_encoder_params = dict(n_components=2)
+    categorical_encoder = "passthrough"  # "onehot"
+    cat_encoder_params = dict()  # dict(handle_unknown="ignore", sparse=False)
+
+    def _transform_feature(
+        self, column_transform_info: FeatureInfo, feature: pd.Series
+    ) -> pd.DataFrame:
+        if column_transform_info.feature_type == "discrete":
+            return super()._transform_feature(column_transform_info, feature)
+        bgm = column_transform_info.transform
+        out = bgm.transform(feature)
+        return pd.DataFrame(
+            out.values[:, 1:].argmax(axis=1), columns=[bgm.feature_name_in]
+        )
+
+    def _inverse_transform_feature(
+        self, column_transform_info: FeatureInfo, column_data: pd.DataFrame
+    ) -> pd.Series:
+        if column_transform_info == "discrete":
+            return super()._inverse_transform_feature(
+                column_transform_info, column_data
+            )
+        bgm = column_transform_info.transform
+        components = column_data.values.reshape(-1)
+        features = bgm.means[components]
+        return pd.Series(features, name=bgm.feature_name_in)
+
+
 class TimeSeriesTabularEncoder(TransformerMixin, BaseEstimator):
     """TimeSeries Tabular encoder.
 
     Model continuous columns with a BayesianGMM and normalized to a scalar [0, 1] and a vector.
     Discrete columns are encoded using a scikit-learn OneHotEncoder.
     """
 
@@ -668,24 +549,26 @@
     """
 
     @validate_arguments(config=dict(arbitrary_types_allowed=True))
     def __init__(
         self,
         max_clusters: int = 10,
         categorical_limit: int = 10,
+        continuous_encoder: str = "gmm",
     ) -> None:
         """Create a data transformer.
 
         Args:
             max_clusters (int):
                 Maximum number of Gaussian distributions in Bayesian GMM.
         """
         self.encoder = BinEncoder(
             max_clusters=max_clusters,
             categorical_limit=categorical_limit,
+            continuous_encoder=continuous_encoder,
         )
 
     def _prepare(
         self,
         static_data: pd.DataFrame,
         temporal_data: List[pd.DataFrame],
         observation_times: List,
```

## synthcity/plugins/core/models/ts_model.py

```diff
@@ -16,15 +16,17 @@
 from tsai.models.TCN import TCN
 from tsai.models.TransformerModel import TransformerModel
 from tsai.models.XceptionTime import XceptionTime
 from tsai.models.XCM import XCM
 
 # synthcity absolute
 import synthcity.logger as log
-from synthcity.plugins.core.models.mlp import MLP, MultiActivationHead, get_nonlin
+from synthcity.plugins.core.models.factory import get_nonlin
+from synthcity.plugins.core.models.layers import MultiActivationHead
+from synthcity.plugins.core.models.mlp import MLP
 from synthcity.utils.constants import DEVICE
 from synthcity.utils.reproducibility import enable_reproducible_results
 from synthcity.utils.samplers import ImbalancedDatasetSampler
 
 modes = [
     "LSTM",
     "GRU",
```

## synthcity/plugins/core/models/tabular_ddpm/__init__.py

```diff
@@ -6,15 +6,15 @@
 # third party
 import numpy as np
 import pandas as pd
 import torch
 from pydantic import validate_arguments
 from torch import nn
 from torch.utils.data import DataLoader, TensorDataset
-from tqdm import tqdm
+from tqdm import trange
 
 # synthcity absolute
 from synthcity.logger import info
 from synthcity.metrics.weighted_metrics import WeightedMetrics
 from synthcity.utils.callbacks import Callback
 from synthcity.utils.constants import DEVICE
 from synthcity.utils.dataframe import discrete_columns
@@ -34,18 +34,17 @@
         num_timesteps: int = 1000,
         is_classification: bool = False,
         gaussian_loss_type: str = "mse",
         scheduler: str = "cosine",
         callbacks: Sequence[Callback] = (),
         device: torch.device = DEVICE,
         log_interval: int = 10,
-        print_interval: int = 100,
         # model params
         model_type: str = "mlp",
-        mlp_params: Optional[dict] = None,
+        model_params: Optional[dict] = None,
         dim_embed: int = 128,
         # early stopping
         n_iter_min: int = 100,
         patience: int = 5,
         patience_metric: Optional[WeightedMetrics] = None,
     ) -> None:
         super().__init__()
@@ -80,34 +79,26 @@
         if self.is_classification and cond is not None:
             if np.ndim(cond) != 1:
                 raise ValueError("cond must be a 1D array")
             self.n_classes = cond.nunique()
         else:
             self.n_classes = 0
 
+        self.feature_names = X.columns
         cat_cols = discrete_columns(X, return_counts=True)
 
         if cat_cols:
-            ini_cols = X.columns
             cat_cols, cat_counts = zip(*cat_cols)
+            num_cols = X.columns.difference(cat_cols)
             # reorder the columns so that the categorical ones go to the end
-            X = X[np.hstack([X.columns[~X.keys().isin(cat_cols)], cat_cols])]
-            cur_cols = X.columns
-            # find the permutation from the reordered columns to the original ones
-            self._col_perm = np.argsort(cur_cols)[np.argsort(np.argsort(ini_cols))]
+            X = X[list(num_cols) + list(cat_cols)]
+            self.feature_names_out = X.columns
         else:
             cat_counts = [0]
-            self._col_perm = np.arange(X.shape[1])
-
-        model_params = dict(
-            num_classes=self.n_classes,
-            use_label=cond is not None,
-            mlp_params=self.mlp_params,
-            dim_emb=self.dim_embed,
-        )
+            self.feature_names_out = self.feature_names
 
         dataset = TensorDataset(
             torch.tensor(X.values, dtype=torch.float32, device=self.device),
             torch.tensor([torch.nan] * len(X), dtype=torch.float32, device=self.device)
             if cond is None
             else torch.tensor(
                 cond.values,
@@ -116,19 +107,22 @@
             ),
         )
 
         self.dataloader = DataLoader(dataset, batch_size=self.batch_size)
 
         self.diffusion = GaussianMultinomialDiffusion(
             model_type=self.model_type,
-            model_params=model_params,
+            model_params=self.model_params,
             num_categorical_features=cat_counts,
             num_numerical_features=X.shape[1] - len(cat_cols),
             gaussian_loss_type=self.gaussian_loss_type,
             num_timesteps=self.num_timesteps,
+            num_classes=self.n_classes,
+            conditional=cond is not None,
+            dim_emb=self.dim_embed,
             scheduler=self.scheduler,
             device=self.device,
         ).to(self.device)
 
         self.ema_model = deepcopy(self.diffusion.denoise_fn)
         for param in self.ema_model.parameters():
             param.detach_()
@@ -136,22 +130,23 @@
         self.optimizer = torch.optim.AdamW(
             self.diffusion.parameters(), lr=self.lr, weight_decay=self.weight_decay
         )
 
         for cbk in self.callbacks:
             cbk.on_fit_begin(self)
 
-        self.loss_history = pd.DataFrame(columns=["step", "mloss", "gloss", "loss"])
+        self.loss_history = []
 
         steps = 0
         curr_loss_multi = 0.0
         curr_loss_gauss = 0.0
         curr_count = 0
+        pbar = trange(self.n_iter, desc="Epoch", leave=True)
 
-        for epoch in tqdm(range(self.n_iter)):
+        for epoch in pbar:
             self.epoch = epoch + 1
             self.diffusion.train()
 
             [cbk.on_epoch_begin(self, epoch) for cbk in self.callbacks]
 
             for x, y in self.dataloader:
                 self.optimizer.zero_grad()
@@ -167,45 +162,49 @@
                 curr_loss_multi += loss_multi.item() * len(x)
                 curr_loss_gauss += loss_gauss.item() * len(x)
 
                 steps += 1
                 if steps % self.log_interval == 0:
                     mloss = np.around(curr_loss_multi / curr_count, 4)
                     gloss = np.around(curr_loss_gauss / curr_count, 4)
-                    if steps % self.print_interval == 0:
-                        info(
-                            f"Step {steps}: MLoss: {mloss} GLoss: {gloss} Sum: {mloss + gloss}"
-                        )
-                    self.loss_history.loc[len(self.loss_history)] = [
-                        steps,
-                        mloss,
-                        gloss,
-                        mloss + gloss,
-                    ]
+                    loss = mloss + gloss
+                    self.loss_history.append(
+                        [
+                            steps,
+                            mloss,
+                            gloss,
+                            loss,
+                        ]
+                    )
                     curr_count = 0
                     curr_loss_gauss = 0.0
                     curr_loss_multi = 0.0
+                    pbar.set_postfix(loss=loss)
 
                 self._update_ema(
                     self.ema_model.parameters(), self.diffusion.parameters()
                 )
 
             self.eval()
 
             try:
                 [cbk.on_epoch_end(self, epoch) for cbk in self.callbacks]
             except StopIteration:
                 info(f"Early stopped at epoch {epoch}")
                 break
 
+        self.loss_history = pd.DataFrame(
+            self.loss_history, columns=["step", "mloss", "gloss", "loss"]
+        ).set_index("step")
+
         for cbk in self.callbacks:
             cbk.on_fit_end(self)
 
         return self
 
-    def generate(self, count: int, cond: Any = None) -> np.ndarray:
+    def generate(self, count: int, cond: Any = None) -> pd.DataFrame:
         self.diffusion.eval()
         if cond is not None:
             cond = torch.tensor(cond, dtype=torch.long, device=self.device)
         sample = self.diffusion.sample_all(count, cond).detach().cpu().numpy()
-        sample = sample[:, self._col_perm]
-        return sample
+        df = pd.DataFrame(sample, columns=self.feature_names_out)
+        return df[self.feature_names]
```

## synthcity/plugins/core/models/tabular_ddpm/gaussian_multinomial_diffsuion.py

```diff
@@ -14,15 +14,15 @@
 import torch.nn.functional as F
 from torch import Tensor
 
 # synthcity absolute
 from synthcity.logger import debug, info, warning
 
 # synthcity relative
-from .modules import MLPDiffusion, ResNetDiffusion
+from .modules import DiffusionModel
 from .utils import (
     discretized_gaussian_log_likelihood,
     index_to_log_onehot,
     log_1_min_a,
     log_add_exp,
     log_categorical,
     mean_flat,
@@ -63,19 +63,23 @@
     else:
         raise NotImplementedError(f"unknown beta schedule: {schedule_name}")
 
 
 class GaussianMultinomialDiffusion(torch.nn.Module):
     def __init__(
         self,
+        *,
         num_numerical_features: int,
         num_categorical_features: tuple,
-        model_type: str = "mlp",
-        model_params: Optional[dict] = None,
+        model_type: str,
+        model_params: dict,
         num_timesteps: int = 1000,
+        num_classes: int = 0,
+        conditional: bool = False,
+        dim_emb: int = 128,
         gaussian_loss_type: str = "mse",
         gaussian_parametrization: str = "eps",
         multinomial_loss_type: str = "vb_stochastic",
         parametrization: str = "x0",
         scheduler: str = "cosine",
         device: torch.device = torch.device("cpu"),
     ) -> None:
@@ -106,32 +110,22 @@
 
         self.slices_for_classes = [np.arange(self.num_classes[0])]
         offsets = np.cumsum(self.num_classes)
         for i in range(1, len(offsets)):
             self.slices_for_classes.append(np.arange(offsets[i - 1], offsets[i]))
         self.offsets = torch.from_numpy(np.append([0], offsets)).to(device).long()
 
-        if model_params is None:
-            model_params = dict(
-                dim_in=self.dim_input, num_classes=0, use_label=False, mlp_params=None
-            )
-        else:
-            model_params["dim_in"] = self.dim_input
-
-        if model_params["mlp_params"] is None:
-            model_params["mlp_params"] = dict(
-                n_units_hidden=256, n_layers_hidden=3, dropout=0.0
-            )
-
-        if model_type == "mlp":
-            self.denoise_fn = MLPDiffusion(**model_params)
-        elif model_type == "resnet":
-            self.denoise_fn = ResNetDiffusion(**model_params)
-        else:
-            raise NotImplementedError(f"unknown model type: {model_type}")
+        self.denoise_fn = DiffusionModel(
+            dim_in=self.dim_input,
+            dim_emb=dim_emb,
+            num_classes=num_classes,
+            conditional=conditional,
+            model_type=model_type,
+            model_params=model_params,
+        )
 
         self.gaussian_loss_type = gaussian_loss_type
         self.gaussian_parametrization = gaussian_parametrization
         self.multinomial_loss_type = multinomial_loss_type
         self.num_timesteps = num_timesteps
         self.parametrization = parametrization
         self.scheduler = scheduler
@@ -154,30 +148,32 @@
         sqrt_recip_alphas_cumprod = np.sqrt(1.0 / alphas_cumprod)
         sqrt_recipm1_alphas_cumprod = np.sqrt(1.0 / alphas_cumprod - 1)
 
         # Gaussian diffusion
 
         self.posterior_variance = (
             betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)
-        )
+        ).to(device)
 
         self.posterior_log_variance_clipped = (
             torch.from_numpy(
                 np.log(
                     np.append(self.posterior_variance[1], self.posterior_variance[1:])
                 )
             )
             .float()
             .to(device)
         )
+
         self.posterior_mean_coef1 = (
-            (betas * np.sqrt(alphas_cumprod_prev) / (1.0 - alphas_cumprod))
+            ((betas * np.sqrt(alphas_cumprod_prev) / (1.0 - alphas_cumprod)))
             .float()
             .to(device)
         )
+
         self.posterior_mean_coef2 = (
             (
                 (1.0 - alphas_cumprod_prev)
                 * np.sqrt(alphas.numpy())
                 / (1.0 - alphas_cumprod)
             )
             .float()
@@ -257,17 +253,15 @@
     ) -> Tuple[Tensor, Tensor, Tensor]:
         if x_start.shape != x_t.shape:
             raise ValueError("x_start.shape != x_t.shape")
         posterior_mean = (
             perm_and_expand(self.posterior_mean_coef1, t, x_t.shape) * x_start
             + perm_and_expand(self.posterior_mean_coef2, t, x_t.shape) * x_t
         )
-        posterior_variance = perm_and_expand(
-            self.posterior_variance.to(x_start.device), t, x_t.shape
-        )
+        posterior_variance = perm_and_expand(self.posterior_variance, t, x_t.shape)
         posterior_log_variance_clipped = perm_and_expand(
             self.posterior_log_variance_clipped, t, x_t.shape
         )
         if not (
             posterior_mean.shape[0]
             == posterior_variance.shape[0]
             == posterior_log_variance_clipped.shape[0]
@@ -288,20 +282,19 @@
 
         B, C = x.shape[:2]
         if t.shape != (B,):
             raise ValueError("length of t is not equal to batch size")
 
         model_variance = torch.cat(
             [
-                self.posterior_variance[1].unsqueeze(0).to(x.device),
+                self.posterior_variance[1].unsqueeze(0),
                 (1.0 - self.alphas)[1:],
             ],
             dim=0,
         )
-        # model_variance = self.posterior_variance.to(x.device)
         model_log_variance = torch.log(model_variance)
 
         model_variance = perm_and_expand(model_variance, t, x.shape)
         model_log_variance = perm_and_expand(model_log_variance, t, x.shape)
 
         if self.gaussian_parametrization == "eps":
             pred_xstart = self._predict_xstart_from_eps(x_t=x, t=t, eps=model_output)
@@ -945,17 +938,18 @@
     ) -> Tensor:
         if ddim:
             info("Sample using DDIM.")
             sample_fn = self.sample_ddim
         else:
             sample_fn = self.sample
 
-        bs = np.diff([*range(0, num_samples, max_batch_size), num_samples])
+        indices = [*range(0, num_samples, max_batch_size), num_samples]
         all_samples = []
 
-        for b in bs:
-            sample = sample_fn(b, cond)
+        for i, b in enumerate(np.diff(indices)):
+            c = None if cond is None else cond[indices[i] : indices[i + 1]]
+            sample = sample_fn(b, c)
             if torch.any(sample.isnan()).item():
                 raise ValueError("found NaNs in sample")
             all_samples.append(sample)
 
         return torch.cat(all_samples, dim=0)
```

## synthcity/plugins/core/models/tabular_ddpm/modules.py

```diff
@@ -7,15 +7,15 @@
 
 # third party
 import torch
 import torch.optim
 from torch import Tensor, nn
 
 # synthcity absolute
-from synthcity.plugins.core.models.mlp import MLP, get_nonlin
+from synthcity.plugins.core.models.factory import get_model, get_nonlin
 
 
 class TimeStepEmbedding(nn.Module):
     def __init__(
         self,
         dim: int,
         max_period: int = 10000,
@@ -55,64 +55,63 @@
         fs = torch.exp(-math.log(T) / mid * torch.arange(mid, dtype=torch.float32))
         fs = fs.to(timesteps.device)
         args = timesteps[:, None].float() * fs[None]
         emb = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)
         return self.fc(emb)
 
 
-class MLPDiffusion(nn.Module):
-    add_residual = False
-
+class DiffusionModel(nn.Module):
     def __init__(
         self,
         dim_in: int,
         dim_emb: int = 128,
         *,
-        mlp_params: dict = {},
-        use_label: bool = False,
+        model_type: str = "mlp",
+        model_params: dict = {},
+        conditional: bool = False,
         num_classes: int = 0,
         emb_nonlin: Union[str, nn.Module] = "silu",
         max_time_period: int = 10000,
     ) -> None:
         super().__init__()
         self.dim_t = dim_emb
         self.num_classes = num_classes
-        self.has_label = use_label
+        self.has_label = conditional
 
         if isinstance(emb_nonlin, str):
             self.emb_nonlin = get_nonlin(emb_nonlin)
         else:
             self.emb_nonlin = emb_nonlin
 
         self.proj = nn.Linear(dim_in, dim_emb)
         self.time_emb = TimeStepEmbedding(dim_emb, max_time_period)
 
-        if use_label:
+        if conditional:
             if self.num_classes > 0:
                 self.label_emb = nn.Embedding(self.num_classes, dim_emb)
             elif self.num_classes == 0:  # regression
                 self.label_emb = nn.Linear(1, dim_emb)
 
-        self.model = MLP(
-            n_units_in=dim_emb,
-            n_units_out=dim_in,
-            task_type="/",
-            residual=self.add_residual,
-            **mlp_params,
-        )
+        if not model_params:
+            model_params = {}  # avoid changing the default dict
+
+        if model_type == "mlp":
+            if not model_params:
+                model_params = dict(n_units_hidden=256, n_layers_hidden=3, dropout=0.0)
+            model_params.update(n_units_in=dim_emb, n_units_out=dim_in)
+        elif model_type == "tabnet":
+            model_params.update(input_dim=dim_emb, output_dim=dim_in)
+
+        self.model = get_model(model_type, model_params)
 
     def forward(self, x: Tensor, t: Tensor, y: Optional[Tensor] = None) -> Tensor:
         emb = self.time_emb(t)
         if self.has_label:
             if y is None:
-                raise ValueError("y must be provided if use_label is True")
+                raise ValueError("y must be provided if conditional is True")
             if self.num_classes == 0:
-                y = y.resize(-1, 1).float()
+                y = y.reshape(-1, 1).float()
             else:
                 y = y.squeeze().long()
             emb += self.emb_nonlin(self.label_emb(y))
         x = self.proj(x) + emb
         return self.model(x)
-
-
-class ResNetDiffusion(MLPDiffusion):
-    add_residual = True
```

## synthcity/plugins/generic/plugin_ddpm.py

```diff
@@ -11,16 +11,22 @@
 import pandas as pd
 
 # Necessary packages
 from pydantic import validate_arguments
 
 # synthcity absolute
 from synthcity.plugins.core.dataloader import DataLoader
-from synthcity.plugins.core.distribution import CategoricalDistribution, Distribution
+from synthcity.plugins.core.distribution import (
+    Distribution,
+    IntegerDistribution,
+    IntLogDistribution,
+    LogDistribution,
+)
 from synthcity.plugins.core.models.tabular_ddpm import TabDDPM
+from synthcity.plugins.core.models.tabular_encoder import TabularEncoder
 from synthcity.plugins.core.plugin import Plugin
 from synthcity.plugins.core.schema import Schema
 from synthcity.utils.callbacks import Callback
 from synthcity.utils.constants import DEVICE
 
 
 class TabDDPMPlugin(Plugin):
@@ -38,36 +44,32 @@
             Number of epochs for training.
         lr: float = 0.002
             Learning rate.
         weight_decay: float = 1e-4
             L2 weight decay.
         batch_size: int = 1024
             Size of mini-batches.
-        model_type: str = "mlp"
-            Type of model to use. Either "mlp" or "resnet".
         num_timesteps: int = 1000
             Number of timesteps to use in the diffusion process.
         gaussian_loss_type: str = "mse"
             Type of loss to use for the Gaussian diffusion process. Either "mse" or "kl".
         scheduler: str = "cosine"
             The scheduler of forward process variance 'beta' to use. Either "cosine" or "linear".
+        model_type: str = "mlp"
+            Type of diffusion model to use ("mlp", "resnet", or "tabnet").
+        model_params: dict = dict(n_layers_hidden=3, n_units_hidden=256, dropout=0.0)
+            Parameters of the diffusion model. Should be different for different model types.
         device: Any = DEVICE
             Device to use for training.
         callbacks: Sequence[Callback] = ()
             Callbacks to use during training.
         log_interval: int = 100
             Number of iterations between logging.
         print_interval: int = 500
             Number of iterations between printing.
-        n_layers_hidden: int = 3
-            Number of hidden layers in the MLP.
-        dim_hidden: int = 256
-            Number of hidden units per hidden layer in the MLP.
-        dropout: float = 0.0
-            Dropout rate.
         dim_embed: int = 128
             Dimensionality of the embedding space.
         random_state: int
             random seed to use
         workspace: Path.
             Optional Path for caching intermediary results.
         compress_dataset: bool. Default = False.
@@ -91,27 +93,25 @@
         self,
         *,
         is_classification: bool = False,
         n_iter: int = 1000,
         lr: float = 0.002,
         weight_decay: float = 1e-4,
         batch_size: int = 1024,
-        model_type: str = "mlp",
         num_timesteps: int = 1000,
         gaussian_loss_type: str = "mse",
         scheduler: str = "cosine",
         device: Any = DEVICE,
         callbacks: Sequence[Callback] = (),
         log_interval: int = 100,
-        print_interval: int = 500,
-        # model params
-        n_layers_hidden: int = 3,
-        dim_hidden: int = 256,
-        dropout: float = 0.0,
+        model_type: str = "mlp",
+        model_params: dict = {},
         dim_embed: int = 128,
+        continuous_encoder: str = "quantile",
+        cont_encoder_params: dict = {},
         # core plugin arguments
         random_state: int = 0,
         workspace: Path = Path("workspace"),
         compress_dataset: bool = False,
         sampling_patience: int = 500,
         **kwargs: Any
     ) -> None:
@@ -122,36 +122,41 @@
             workspace=workspace,
             compress_dataset=compress_dataset,
             **kwargs
         )
 
         self.is_classification = is_classification
 
-        mlp_params = dict(
-            n_layers_hidden=n_layers_hidden, n_units_hidden=dim_hidden, dropout=dropout
-        )
-
         self.model = TabDDPM(
             n_iter=n_iter,
             lr=lr,
             weight_decay=weight_decay,
             batch_size=batch_size,
             num_timesteps=num_timesteps,
             gaussian_loss_type=gaussian_loss_type,
             is_classification=is_classification,
             scheduler=scheduler,
             device=device,
             callbacks=callbacks,
             log_interval=log_interval,
-            print_interval=print_interval,
             model_type=model_type,
-            mlp_params=mlp_params,
+            model_params=model_params.copy(),
             dim_embed=dim_embed,
         )
 
+        cont_encoder_params = cont_encoder_params.copy()
+        cont_encoder_params.update(random_state=random_state)
+
+        self.encoder = TabularEncoder(
+            continuous_encoder=continuous_encoder,
+            cont_encoder_params=cont_encoder_params,
+            categorical_encoder="none",
+            cat_encoder_params=dict(),
+        )
+
     @staticmethod
     def name() -> str:
         return "ddpm"
 
     @staticmethod
     def type() -> str:
         return "generic"
@@ -170,70 +175,76 @@
         Proportion of samples   Float{0.25, 0.5, 1, 2, 4, 8}
         ----------------------------------------------
         Dropout                 0.0
         Scheduler               cosine (Nichol, 2021)
         Gaussian diffusion loss MSE
         """
         return [
-            # TODO: change to loguniform distribution
-            CategoricalDistribution(name="lr", choices=[1e-5, 1e-4, 1e-3, 2e-3, 3e-3]),
-            CategoricalDistribution(name="batch_size", choices=[256, 4096]),
-            CategoricalDistribution(name="num_timesteps", choices=[100, 1000]),
-            CategoricalDistribution(name="n_iter", choices=[5000, 10000, 20000]),
-            CategoricalDistribution(name="n_layers_hidden", choices=[2, 4, 6, 8]),
-            CategoricalDistribution(name="dim_hidden", choices=[128, 256, 512, 1024]),
+            LogDistribution(name="lr", low=1e-5, high=1e-1),
+            IntLogDistribution(name="batch_size", low=256, high=4096),
+            IntegerDistribution(name="num_timesteps", low=10, high=1000),
+            IntLogDistribution(name="n_iter", low=1000, high=10000),
+            # IntegerDistribution(name="n_layers_hidden", low=2, high=8),
+            # IntLogDistribution(name="dim_hidden", low=128, high=1024),
         ]
 
     def _fit(self, X: DataLoader, *args: Any, **kwargs: Any) -> "TabDDPMPlugin":
         """Fit the model to the data.
 
         Optionally, a condition can be given as the keyword argument `cond`.
 
         If the task is classification, the target labels are automatically regarded as the condition, and no additional condition should be given.
 
         If the task is regression, the target variable is not specially treated. There is no condition by default, but can be given by the user, either as a column name or an array-like.
         """
         df = X.dataframe()
         cond = kwargs.pop("cond", None)
-
-        # note that the TabularEncoder is not used in this plugin, because the
-        # Gaussian multinomial diffusion module needs to know the number of classes
-        # for each discrete feature before it applies torch.nn.functional.one_hot
-        # on these features, and it also preprocesses the continuous features differently.
+        self.loss_history = None
 
         if args:
             raise ValueError("Only keyword arguments are allowed")
 
         if self.is_classification:
             if cond is not None:
                 raise ValueError(
                     "cond is already given by the labels for classification"
                 )
-            _, cond = X.unpack()
+            df, cond = X.unpack()
             self._labels, self._cond_dist = np.unique(cond, return_counts=True)
             self._cond_dist = self._cond_dist / self._cond_dist.sum()
-        else:
-            if type(cond) is str:
-                cond = df[cond]
+            self.target_name = cond.name
+
+        df = self.encoder.fit_transform(df)
 
         if cond is not None:
+            if type(cond) is str:
+                cond = df[cond]
             cond = pd.Series(cond, index=df.index)
+            self.expecting_conditional = True
 
         # NOTE: cond may also be included in the dataframe
         self.model.fit(df, cond, **kwargs)
+        self.loss_history = self.model.loss_history
 
         return self
 
     def _generate(self, count: int, syn_schema: Schema, **kwargs: Any) -> DataLoader:
         cond = kwargs.pop("cond", None)
 
         if self.is_classification and cond is None:
             # randomly generate labels following the distribution of the training data
             cond = np.random.choice(self._labels, size=count, p=self._cond_dist)
 
+        if cond is not None and len(cond) > count:
+            raise ValueError("The length of cond is less than the required count")
+
         def callback(count):  # type: ignore
-            return self.model.generate(count, cond=cond)
+            df = self.model.generate(count, cond=cond)
+            df = self.encoder.inverse_transform(df)
+            if self.is_classification:
+                df = df.join(pd.Series(cond, name=self.target_name))
+            return df
 
         return self._safe_generate(callback, count, syn_schema, **kwargs)
 
 
 plugin = TabDDPMPlugin
```

## synthcity/plugins/time_series/plugin_fflows.py

```diff
@@ -7,28 +7,28 @@
 
 # third party
 import numpy as np
 import pandas as pd
 from fflows import FourierFlow
 
 # synthcity absolute
+from synthcity.plugins import Plugins
 from synthcity.plugins.core.dataloader import DataLoader
 from synthcity.plugins.core.distribution import (
     CategoricalDistribution,
     Distribution,
     IntegerDistribution,
 )
 from synthcity.plugins.core.models.tabular_encoder import (
     TabularEncoder,
     TimeSeriesTabularEncoder,
 )
 from synthcity.plugins.core.models.ts_model import TimeSeriesModel
 from synthcity.plugins.core.plugin import Plugin
 from synthcity.plugins.core.schema import Schema
-from synthcity.plugins.generic import GenericPlugins
 from synthcity.utils.constants import DEVICE
 
 
 class FourierFlowsPlugin(Plugin):
     """
     .. inheritance-diagram:: synthcity.plugins.time_series.plugin_fflows.FourierFlowsPlugin
         :parts: 1
@@ -130,17 +130,15 @@
             hidden=n_units_hidden,
             n_flows=n_flows,
             FFT=FFT,
             flip=flip,
             normalize=normalize,
         ).to(device)
 
-        self.static_model = GenericPlugins().get(
-            self.static_model_name, device=self.device
-        )
+        self.static_model = Plugins().get(self.static_model_name, device=self.device)
 
         self.temporal_encoder = TimeSeriesTabularEncoder(
             max_clusters=encoder_max_clusters
         )
         self.outcome_encoder = TabularEncoder(max_clusters=encoder_max_clusters)
 
     @staticmethod
```

## Comparing `synthcity-0.2.5.dist-info/LICENSE` & `synthcity-0.2.6.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `synthcity-0.2.5.dist-info/METADATA` & `synthcity-0.2.6.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,24 +1,25 @@
 Metadata-Version: 2.1
 Name: synthcity
-Version: 0.2.5
+Version: 0.2.6
 Summary: Synthetic data generator and evaluator!
 Home-page: UNKNOWN
 License: Apache-2.0 license
 Platform: any
 Classifier: Programming Language :: Python :: 3
 Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
 Classifier: Intended Audience :: Science/Research
 Classifier: Operating System :: OS Independent
 Requires-Python: >=3.7
 Description-Content-Type: text/markdown; charset=UTF-8
+License-File: LICENSE
 Requires-Dist: scikit-learn (>=1.0)
 Requires-Dist: nflows (>=0.14)
 Requires-Dist: pandas (<2.0,>=1.3)
-Requires-Dist: torch (<2.0,>=1.10)
+Requires-Dist: torch (<2.0,>=1.10.0)
 Requires-Dist: numpy (>=1.20)
 Requires-Dist: lifelines (>=0.27)
 Requires-Dist: opacus (>=1.3)
 Requires-Dist: decaf-synthetic-data (>=0.1.6)
 Requires-Dist: optuna (>=3.1)
 Requires-Dist: shap
 Requires-Dist: tqdm
```

## Comparing `synthcity-0.2.5.dist-info/RECORD` & `synthcity-0.2.6.dist-info/RECORD`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 synthcity/__init__.py,sha256=NyaA-ra5ttr6nEwYWHq7_K6zw51XPP1kSjTx3FISCaA,627
 synthcity/logger.py,sha256=Zfchswe5eTEYkmV_Fhxzxx4_wLZioa72YmXxrGpdXNI,2765
-synthcity/version.py,sha256=fc8_txmeDM3t0MdPc_M1s_agTOsloPvglTu3AN_xAIY,120
+synthcity/version.py,sha256=YC7DQC2mvzNncH0PM3GJL3Ruc37VGgWtTL8QQP8pR-w,120
 synthcity/benchmark/__init__.py,sha256=E6r2OiyRxnbwb5Tf51RS2Ah5a7Qjpdl7AQH5v5Kyj9c,15727
 synthcity/benchmark/utils.py,sha256=sp1G0cN78r5ovdov6s6Xq37zM0RProocHzku0a9qP8Q,8853
 synthcity/metrics/__init__.py,sha256=Twrr4mO-EoQubby9A236m5BmzXqfV7tesRdVnsgpB7s,121
 synthcity/metrics/_utils.py,sha256=OYfZ-qApoK5GXVq6yvC2g1cwfjtFQYnEaML8UfV-ut4,3757
 synthcity/metrics/eval.py,sha256=3SDNPW-2_QAkTiyqyFtkVwGY3SDcAamInVFUPDXZ4t4,7885
 synthcity/metrics/eval_attacks.py,sha256=7vBcun80SXKBJa4v8zhb-OnBotB_zb2rTLAC_pmz0II,6118
 synthcity/metrics/eval_detection.py,sha256=HZL899a-9-YLoesSIEcADWjBynXDxnhccL2SibdsrVA,10365
@@ -19,38 +19,41 @@
 synthcity/metrics/core/metric.py,sha256=sSiXgXHZqJ0CREoQyNv9-h4dZFsQv0pCwt3P8_aY8xo,4397
 synthcity/metrics/representations/OneClass.py,sha256=cYY30XQCFQiNg-Q5QfgVxjMabvzw29VSpTEr8eTgAM4,5674
 synthcity/metrics/representations/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 synthcity/metrics/representations/networks.py,sha256=6h282CPdrwaTjG_OVae25fBr2gpJRJD7fuCD6bD4Yi4,2164
 synthcity/plugins/__init__.py,sha256=daG7BEPjBbPEdtfh0Gz65dc-BCvei_9wbEAN6mrdAB8,892
 synthcity/plugins/core/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 synthcity/plugins/core/constraints.py,sha256=ydaxeA8_dQCRceG0763F_04smpHHG6Vu56pyecN92-I,10277
-synthcity/plugins/core/dataloader.py,sha256=nvdRHmLjhiWb5_0csv-IPw2z0w_uHgqRpizfVHad9PU,60621
+synthcity/plugins/core/dataloader.py,sha256=ijp_IOMZz05L5wvgSSO6PD0djjt7QyLozQTgPypLRyg,60624
 synthcity/plugins/core/dataset.py,sha256=Tvzqe9yLIv-UwgYBjxyEjfy4RavSOggagFYTLpTLsfc,4995
-synthcity/plugins/core/distribution.py,sha256=8Xd5dHPFPbE4I0BmNJxpcxA5PJsTBilNBd5a-Y1jXf8,13163
+synthcity/plugins/core/distribution.py,sha256=3nVZCVFqclPUmS1qjSAA7Vr-xFmi19TVDSLO08kYw_s,13845
 synthcity/plugins/core/plugin.py,sha256=Xzoit9eP8FUrtPvVAmqjnw2LznxmPX0oApI-yI78S_c,25106
-synthcity/plugins/core/schema.py,sha256=f-wk0XuOK3pv31o5kbEq_URbSX4A9hT3PyhQMAVFNh8,7594
+synthcity/plugins/core/schema.py,sha256=JnzoaYiQ9ZcVlCOHScTMazI160W-ikASkQzFsJH3X5U,7608
 synthcity/plugins/core/serializable.py,sha256=0-OowNRq21Tz1x9M2iuFFoM5QzBAGsEMnjhxkpzv1Hg,4417
 synthcity/plugins/core/models/RGCNConv.py,sha256=Hv6uOmaUuN1XhYT6gy-r7u_DUG0NoSR5EGkx8Tzrb2c,11889
 synthcity/plugins/core/models/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-synthcity/plugins/core/models/convnet.py,sha256=c9Mter1Z9Vsic8uBVCHTjceeBzy0_kBvTsPDadBBIOY,19210
-synthcity/plugins/core/models/data_encoder.py,sha256=7tFM6EfNh3EWV3UQG8R59uf09wDOEpLrl2IyN6ZS81A,3666
+synthcity/plugins/core/models/convnet.py,sha256=Z60StVyrUPrKe-0pRwYQfJYynLZfxsN-vr4K8-v4wZM,19210
+synthcity/plugins/core/models/factory.py,sha256=8QBEeeH9DiqZ3KQYW3Gm_OGyqr37hj_DvAFaHKCYNtI,3848
+synthcity/plugins/core/models/feature_encoder.py,sha256=GSD77Sk0PTZTaWL6yeM27Y0ZtJWphFMxuqGrKPMUWE0,9270
 synthcity/plugins/core/models/flows.py,sha256=HZcPCwwC5rqdmJa3LpuY1tshcF5q1XIGOopVXDsMqM8,16293
+synthcity/plugins/core/models/functions.py,sha256=5rqXK2Qu611RsUt-5nrbGK0wGISqX0O3TuxtFNbNVoM,4884
 synthcity/plugins/core/models/gan.py,sha256=Y76FLxnNLPqtqPkVteKiPcf74zkVJY35CyBYLSJ9SEo,27155
-synthcity/plugins/core/models/goggle.py,sha256=ABpjdhadqKoLPQeX4xdUDclzx3Ajz5LQFHc4EuWf4fw,25132
+synthcity/plugins/core/models/goggle.py,sha256=lQ2Vv7sr3tBYesgUf3HtExV7CiqaK7mocm7AHic2q00,25119
 synthcity/plugins/core/models/image_gan.py,sha256=C5ffO3meoDqiOWAH3Y5Qajx4s7Mm6KECUUT3PFZejLw,25246
-synthcity/plugins/core/models/layers.py,sha256=3Vp0UbFUH6yg2jxg_hkG7jHzD5qr4zRUeoVQguPMm_k,1008
-synthcity/plugins/core/models/mlp.py,sha256=Fu3tDU9K6avgQykQHDNrJLdylXDwQqo248LiqIhVFkY,15212
-synthcity/plugins/core/models/tabular_encoder.py,sha256=e9Xn3KUk6WFFrCgpgvE764bcpvLx9LI_EGqRZ1eeoUY,25471
+synthcity/plugins/core/models/layers.py,sha256=7v6pEOM3KC9EdesqGZxKLVRwS_688ypUnCCgzjURtO4,5209
+synthcity/plugins/core/models/mlp.py,sha256=4UXMUvOMvSIPAV14QZoHxF--isBsGK6kTPU_3GncfOE,11431
+synthcity/plugins/core/models/tabnet.py,sha256=rRoHeIsv8nWSwVufmyGlI4rF8G7vu5Xa76A1pb89a-Y,17756
+synthcity/plugins/core/models/tabular_encoder.py,sha256=0UgolYDOm2vGUKrLjkFVO2aU_nRG5YI2Rq8sNHijLIY,22043
 synthcity/plugins/core/models/tabular_flows.py,sha256=qzoUqpOY0N6Vx_uoAjKMwQ3yqDJxfbO0lk2QtSsLRnw,6440
 synthcity/plugins/core/models/tabular_gan.py,sha256=IHmC4a_1_qVILNH-mGi6nDIABSI1tUx9aK7a_VQkEzY,18358
 synthcity/plugins/core/models/tabular_goggle.py,sha256=bYMDmYrpp1P_Rz4F3MvEjneupXD2jCT74yHwroxCa6I,9703
 synthcity/plugins/core/models/tabular_vae.py,sha256=4Seadtsc8fF-3idLYvJyQe5B4wx3SpYXgUulINNoXn4,11061
 synthcity/plugins/core/models/transformer.py,sha256=mugx8JeyNIEJaKvGAMMKdKZAdtFQXF949_NdGdWVwqg,2051
 synthcity/plugins/core/models/ts_gan.py,sha256=PmCfglIE3x_qHP2bW_1qOVIEwsl5Ipa2hJPEsA9JXBU,31108
-synthcity/plugins/core/models/ts_model.py,sha256=JM_2GNiekiQRPOtp-HMluPx1q8iH-dm479Zz-rzjQXA,26260
+synthcity/plugins/core/models/ts_model.py,sha256=oA_b6PU1nsKQX3BxzoU5iQqVPRiyhCH19iqZuCjs-ao,26357
 synthcity/plugins/core/models/ts_tabular_gan.py,sha256=toiWcW_YGfknRHrmBlptpuQLzQksuHJpHo8ugwt2nL0,12754
 synthcity/plugins/core/models/ts_tabular_vae.py,sha256=kpT-jt4pu6_fBEqPIJDoj0YfKAKew4OTOtQgMudnhWo,9637
 synthcity/plugins/core/models/ts_vae.py,sha256=ahmTLxfR2adb68H4gd42WP4IUUGLouCOHUScIP2LbVI,21111
 synthcity/plugins/core/models/vae.py,sha256=RTU6pMcslOUScJ_Hmq9b5JRL2RIUVBiIin7iSozh-Mo,20720
 synthcity/plugins/core/models/dag/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 synthcity/plugins/core/models/dag/data.py,sha256=vG2m1H4uT3RTwI4ExDfXm9N5LYLvydU4UA0mlCX4FYo,2083
 synthcity/plugins/core/models/dag/dsl.py,sha256=w154U20R0idIdXjUdbkzYBRVBj03gC9FmrJOhXFGYgw,7343
@@ -65,17 +68,17 @@
 synthcity/plugins/core/models/survival_analysis/surv_coxph.py,sha256=FVO5ShagH9snA7lpjfxsXsFKb7oj3IMyNg3vCjUWJ5M,2597
 synthcity/plugins/core/models/survival_analysis/surv_deephit.py,sha256=hCaV4qjiZ8VoCx6aqWfY_1GqcE20ZTzEzEtmG2aVXi4,5214
 synthcity/plugins/core/models/survival_analysis/surv_xgb.py,sha256=WOdGlLVPoDEuj37iDeO2p8daHBOQ65HDIhiBnUGZnYU,5954
 synthcity/plugins/core/models/survival_analysis/third_party/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 synthcity/plugins/core/models/survival_analysis/third_party/metrics.py,sha256=olvmvrBFa2D-D_0HY6F6nSePeUIOaAK1InJN8IRoMOI,14800
 synthcity/plugins/core/models/survival_analysis/third_party/nonparametric.py,sha256=UjbRFu5SQnuTcC5X8JFX0EE_iOJzfNVuaZN1-NwrYLw,11564
 synthcity/plugins/core/models/survival_analysis/third_party/util.py,sha256=X-QZRYUYswKbI1nxlzqd5JMXuyOcA-06l85Xgpgz2uo,9513
-synthcity/plugins/core/models/tabular_ddpm/__init__.py,sha256=bk5Qu6jKnHbO-ToWPQsl4d1etFW_27CWoeo5H1ag6yo,7380
-synthcity/plugins/core/models/tabular_ddpm/gaussian_multinomial_diffsuion.py,sha256=nzkGZ7jSipGvplaXTKOHAs-86cvBPN0e9eJOLy_qBPg,34804
-synthcity/plugins/core/models/tabular_ddpm/modules.py,sha256=FMiNiPnvyasIHyuKv1DMvaVVp1zGDxYEPHx5PD9eoic,3424
+synthcity/plugins/core/models/tabular_ddpm/__init__.py,sha256=DeldVdm1Oc5I_5LjKtj-f5OmuG-7v7DghBV0ribUsKc,7249
+synthcity/plugins/core/models/tabular_ddpm/gaussian_multinomial_diffsuion.py,sha256=YuU6B6rrEdGX5cSR3-nVglRu4eYcy3-ulI4qOmBYbX8,34437
+synthcity/plugins/core/models/tabular_ddpm/modules.py,sha256=U_J_e-A8WzRI-FnWIdqMVP_dn1PCTTNSL_JxvhTjfiU,3683
 synthcity/plugins/core/models/tabular_ddpm/utils.py,sha256=4mTUy3_yNzvpzF5FU7oW57ruE9KpCos1rJakel3S8Ek,5250
 synthcity/plugins/core/models/time_series_survival/__init__.py,sha256=W3C0jekhvLLkQONd8zqjW4T5r1sIHQkXJ5C6shRNUws,432
 synthcity/plugins/core/models/time_series_survival/_base.py,sha256=ozy7I41jdlHInkpdgwvIZnGgmZw2ifBhiiAjxqu-qCM,1738
 synthcity/plugins/core/models/time_series_survival/benchmarks.py,sha256=wnDQQ1NDNUys95N5TPKJ7cvOVj3Uiz50Vc7TTzjc0zw,12847
 synthcity/plugins/core/models/time_series_survival/loader.py,sha256=BqTFLnbj9rqtsk7Apgzn6bxd84jevYGKR0F1mT_J6ow,552
 synthcity/plugins/core/models/time_series_survival/ts_surv_coxph.py,sha256=LN5Ri_m114Dxpf5t0FSDWe4FnszAsn6H3BI1kbST8fU,3578
 synthcity/plugins/core/models/time_series_survival/ts_surv_dynamic_deephit.py,sha256=GVKtzS1c6ZqVec-Ay7-vQYwaN3L8CNVyobvUv-Hpzn8,25693
@@ -95,15 +98,15 @@
 synthcity/plugins/core/models/time_to_event/tte_tenn.py,sha256=OBsrRzeWYr0hpVeDxhqYP4bSgcih2kjYBLsrbAdhCY0,14973
 synthcity/plugins/core/models/time_to_event/tte_xgb.py,sha256=17pXY5DBIdsQIbS2GVWm4zQt_l34V8zHRSblDhTOum0,4816
 synthcity/plugins/domain_adaptation/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 synthcity/plugins/domain_adaptation/plugin_radialgan.py,sha256=DkPU_CmEkVdV3oEpRYPVYVpAz9vdpyZmbnq3p78mYX8,34278
 synthcity/plugins/generic/__init__.py,sha256=aJ06J1U2Fu55vtiFIo9NDojvgpEpDadph9xUr5DL-Tc,451
 synthcity/plugins/generic/plugin_bayesian_network.py,sha256=-y12bF3g9DfkczEDyAIFhpmxIfQ3YFtu31Ax0vWLuTo,6830
 synthcity/plugins/generic/plugin_ctgan.py,sha256=qsFQBX-8EcpEuLEcB4PviI0FT5jXnHrsxng8k9GIT9Q,11636
-synthcity/plugins/generic/plugin_ddpm.py,sha256=TVm2JemfVJuBnuRSUVEITcEgpP6TUn7JqhtXPSNKlGo,8814
+synthcity/plugins/generic/plugin_ddpm.py,sha256=tXRzMikPojWLAiXjvFl0EmfGTh2LNJQY5oZXTuID_6Y,9120
 synthcity/plugins/generic/plugin_dummy_sampler.py,sha256=c_e5sVxpuiGEymO--Hl2ZzfajYxEQ_wPyoLw0EdSbW0,1964
 synthcity/plugins/generic/plugin_goggle.py,sha256=byGYbrbAruOJ-YhIpbXq_b5MYU6cObOZ4T3bNOyKxSs,10298
 synthcity/plugins/generic/plugin_marginal_distributions.py,sha256=VUEDBLK_DW4v474M-EdPsAhIWy0RklJ0WXatrab_JkI,2028
 synthcity/plugins/generic/plugin_nflow.py,sha256=JiBCe0e4oO1y7P36-i8frGjRvCdrh6qS4Dp5glSFNz4,11759
 synthcity/plugins/generic/plugin_rtvae.py,sha256=LmbRqfn2QRuJQzgGQJEEDKrOI6giJfJPOxS3w93mQ8Y,8920
 synthcity/plugins/generic/plugin_tvae.py,sha256=-1IaPwLQjFIbxxOF-dvymKW1m6BJ27IOjG-l16_Bk3s,9041
 synthcity/plugins/generic/plugin_uniform_sampler.py,sha256=tORIOarQRXOJm-5EedXEOH8TEkoaQtmWm0wiJk67etg,1943
@@ -119,32 +122,33 @@
 synthcity/plugins/survival_analysis/__init__.py,sha256=EF_APjFRR81dIr5Rq8S7fvqWwOa-Ot2jZrse_UcuM_E,490
 synthcity/plugins/survival_analysis/_survival_pipeline.py,sha256=sBypX2OZMsmddjt9zHvSwWEfO2BOhMD0EoFQbcnYwSI,7534
 synthcity/plugins/survival_analysis/plugin_survae.py,sha256=QEQjTpTDcuY6k5YN5sHwl9i6hYt21YSnVbSgneQVdQI,5873
 synthcity/plugins/survival_analysis/plugin_survival_ctgan.py,sha256=0Lfw1IPyV61zLyrQ0ti9YHtMPButYkCGZcvc5LnlpPI,5943
 synthcity/plugins/survival_analysis/plugin_survival_gan.py,sha256=RBu8Rw-aDUUZpGasLRhWQhzI3MJCi-7K24izUPMZUxM,8082
 synthcity/plugins/survival_analysis/plugin_survival_nflow.py,sha256=dyZkQUaxpsEaUdn2A2KWsTqy4F2hS7XhFvmhQLnrea4,5626
 synthcity/plugins/time_series/__init__.py,sha256=cC0NuC22dcJxKoge1cEEINXvfNBRhwn-qOvo2NSJvW4,472
-synthcity/plugins/time_series/plugin_fflows.py,sha256=sZH8tpcO7XnkIqcfpT2-Ksr4-wOAZjl8m7nTLcjAf50,10607
+synthcity/plugins/time_series/plugin_fflows.py,sha256=dcgSbyePvSyZCEE2iNyGVP8vCienqTchrVBq6Y-PPEA,10563
 synthcity/plugins/time_series/plugin_timegan.py,sha256=W7NN4VqbjVrq1coCiUSuhzS9jspAtXLNtGvEWa3E3Ng,18792
 synthcity/plugins/time_series/plugin_timevae.py,sha256=SFYsrqdDhsT06-FV9AKDjB_jP7KQPOiVKUVMOppNH9o,13933
 synthcity/utils/anonymization.py,sha256=Azca0Q9Dat7Po_wREuYnLNeNyiCZ-4li1jypBQUr2EQ,12146
 synthcity/utils/callbacks.py,sha256=4-SG9tL4ueFUGJxF6dNZRdCeTuScYHcdkE2yiRZbrCg,2691
 synthcity/utils/compression.py,sha256=MEXvpMmpzURC3-wBa2Raov1bl5MFgjw8N-q0tUAJpeo,5705
 synthcity/utils/constants.py,sha256=giecIkrMeOXLpCfjSderrXsDITu2XC09t1Dbh8JFvww,98
 synthcity/utils/dataframe.py,sha256=Mc6LImzln13xv4KXT9frmKtan1Gulml-uUfv6T8N66Y,575
 synthcity/utils/evaluation.py,sha256=TdVuA5xSrpxCAqjvJl9YEBr009IOtO5E_6upvf3E9W0,2775
 synthcity/utils/optimizer.py,sha256=g_8IjInpSd-58pp2olQuvnrRUdeynD88FaXCKispKWo,6482
+synthcity/utils/optuna_sample.py,sha256=iGwWSc23SOZOFVvHZ_iVkd98vWMDNCu121odpdWqfSU,1030
 synthcity/utils/redis_wrapper.py,sha256=37ro-RUEcGnlzLmcqgHt1rc05l6lB-YcRe4eU5d84A0,653
 synthcity/utils/reproducibility.py,sha256=EGQMhxn8E1SLrftoGA1lBNdRdJ_AQrFScYrayT88wvU,473
 synthcity/utils/samplers.py,sha256=ouNoW5wjo8AsmjVnYKg-uYYox3zbyQn1tEzZjWrbgME,11212
 synthcity/utils/serialization.py,sha256=SmtAeFtSTN2i9mHoKGCbbjuj_rrknfcpa6ZucuZlplM,1062
 synthcity/utils/datasets/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 synthcity/utils/datasets/time_series/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 synthcity/utils/datasets/time_series/google_stocks.py,sha256=N4FmCw8uAk2sDuDS-JaAbbL-NisOQyYJmXHzUJCDdSQ,2617
 synthcity/utils/datasets/time_series/pbc.py,sha256=l_oxaV14QPgPlJfiQFMR8mUWs01dTqeajoKXaATn9lQ,5973
 synthcity/utils/datasets/time_series/sine.py,sha256=Gr4j8Fu-keGsN6KmqcERf3gpZ0_8QqXek4tJSwh2Tg0,2722
 synthcity/utils/datasets/time_series/data/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-synthcity-0.2.5.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
-synthcity-0.2.5.dist-info/METADATA,sha256=IlCTfiPEgJEtD1M4xU3mAc90McTVpxAIL3vDmiLzlOo,28920
-synthcity-0.2.5.dist-info/WHEEL,sha256=PSUl63OH0biayav1jofH0slay5qEns64wWZYocYO0c4,108
-synthcity-0.2.5.dist-info/top_level.txt,sha256=fBxOU4no5e1lw7rZalUXkmOiZVH40hE50eExHY5WIkY,10
-synthcity-0.2.5.dist-info/RECORD,,
+synthcity-0.2.6.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
+synthcity-0.2.6.dist-info/METADATA,sha256=yY8mw8dGH3Ktxyl8AdSCtyEcPWAL-YjikNDG8P0G28I,28944
+synthcity-0.2.6.dist-info/WHEEL,sha256=PSUl63OH0biayav1jofH0slay5qEns64wWZYocYO0c4,108
+synthcity-0.2.6.dist-info/top_level.txt,sha256=fBxOU4no5e1lw7rZalUXkmOiZVH40hE50eExHY5WIkY,10
+synthcity-0.2.6.dist-info/RECORD,,
```

